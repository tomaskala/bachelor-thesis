{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "from imp import reload\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "import gensim\n",
    "\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from event_detection import data_fetchers, event_detector, plotting\n",
    "\n",
    "reload(logging)\n",
    "logging.basicConfig(stream=sys.stdout, format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total_time = time()\n",
    "\n",
    "t = time()\n",
    "documents, relative_days = data_fetchers.fetch_czech_corpus_dec_jan()\n",
    "\n",
    "stream_length = max(relative_days) + 1  # Zero-based, hence the + 1.\n",
    "print('Read input in %fs.' % (time() - t))\n",
    "print('Stream length: %d' % stream_length)\n",
    "\n",
    "t = time()\n",
    "vectorizer = CountVectorizer(min_df=30, max_df=100000, binary=True, stop_words=event_detector.CZECH_STOPWORDS)\n",
    "bow_matrix = vectorizer.fit_transform(documents).tocsr()\n",
    "id2word = {v: k for k, v in vectorizer.vocabulary_.items()}\n",
    "print('Created bag of words in %fs.' % (time() - t))\n",
    "print('BOW:', bow_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time bow_matrix = TfidfTransformer().fit_transform(bow_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_TOPICS = 100\n",
    "\n",
    "corpus = gensim.matutils.Sparse2Corpus(bow_matrix, documents_columns=False)\n",
    "dictionary = gensim.corpora.Dictionary.from_corpus(corpus, id2word=id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "LSI_PATH = ('./dec_jan_%d_topics_tfidf.lsi' % NUM_TOPICS)\n",
    "\n",
    "if os.path.exists(LSI_PATH):\n",
    "    lsi = gensim.models.LsiModel.load(LSI_PATH)\n",
    "    print('Loaded %d LSI topics from file' % NUM_TOPICS)\n",
    "else:\n",
    "    %time lsi = gensim.models.LsiModel(corpus, id2word=dictionary, num_topics=NUM_TOPICS, onepass=False, power_iters=5)\n",
    "    lsi.save(LSI_PATH)\n",
    "    print('Generated LSI model for %d topics and saved to file' % NUM_TOPICS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lsi_gensim = gensim.matutils.corpus2dense(lsi[corpus], len(lsi.projection.s)).T / lsi.projection.s\n",
    "normalize(lsi_gensim, norm='l2', copy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lsi.print_topics(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "NUM_CLUSTERS = 15\n",
    "\n",
    "kmeans = MiniBatchKMeans(n_clusters=NUM_CLUSTERS, n_init=10, random_state=1)\n",
    "\n",
    "%time kmeans.fit(lsi_gensim)\n",
    "\n",
    "clusters = [[] for _ in range(NUM_CLUSTERS)]\n",
    "\n",
    "for doc, label in np.ndenumerate(kmeans.labels_):\n",
    "    clusters[label].append(doc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for cluster, docs in enumerate(clusters):\n",
    "    print(cluster, len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def detect_events(X, D, length, inverse, cluster_num):\n",
    "    DPS_BOUNDARY = 0.1\n",
    "    \n",
    "    trajectories = event_detector.construct_feature_trajectories(X, D)\n",
    "    dps, dp = event_detector.spectral_analysis(trajectories)\n",
    "    \n",
    "    # Aperiodic events\n",
    "    aperiodic_indices = np.where((dps > DPS_BOUNDARY) & (dp > math.ceil(length / 2)))[0]\n",
    "    aperiodic_bow = X[:, aperiodic_indices]\n",
    "    aperiodic_features = trajectories[aperiodic_indices, :]\n",
    "    aperiodic_dps = dps[aperiodic_indices]\n",
    "    aperiodic_dp = dp[aperiodic_indices]\n",
    "    \n",
    "    if len(aperiodic_indices > 0):\n",
    "        aperiodic_events = event_detector.unsupervised_greedy_event_detection(aperiodic_indices, aperiodic_bow,\n",
    "                                                                               aperiodic_features, aperiodic_dps,\n",
    "                                                                               aperiodic_dp)\n",
    "        plotting.plot_events(trajectories, aperiodic_events, inverse, dps, dp, dirname='./aperiodic_{}'.format(cluster_num))\n",
    "        print('Aperiodic done')\n",
    "    else:\n",
    "        print('No high power aperiodic features detected')\n",
    "    \n",
    "    # Periodic events\n",
    "    periodic_indices = np.where((dps > DPS_BOUNDARY) & (dp <= math.ceil(length / 2)))[0]\n",
    "    periodic_bow = X[:, periodic_indices]\n",
    "    periodic_features = trajectories[periodic_indices, :]\n",
    "    periodic_dps = dps[periodic_indices]\n",
    "    periodic_dp = dp[periodic_indices]\n",
    "    \n",
    "    if len(periodic_indices > 0):\n",
    "        periodic_events = event_detector.unsupervised_greedy_event_detection(periodic_indices, periodic_bow,\n",
    "                                                                              periodic_features, periodic_dps, periodic_dp)\n",
    "    \n",
    "        plotting.plot_events(trajectories, periodic_events, inverse, dps, dp, dirname='./periodic_{}'.format(cluster_num))\n",
    "        print('Periodic done')\n",
    "    else:\n",
    "        print('No high power periodic features detected')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for cluster, docs_indices in enumerate(clusters):\n",
    "    print('---------- Cluster {:d} ----------'.format(cluster))\n",
    "    cluster_docs = [documents[doc_id] for doc_id in docs_indices]\n",
    "    cluster_days = [relative_days[doc_id] for doc_id in docs_indices]\n",
    "    \n",
    "    cluster_stream_len = max(cluster_days) + 1  # Zero-based, hence the + 1.\n",
    "\n",
    "    t = time()\n",
    "    vectorizer = CountVectorizer(min_df=30, max_df=0.9, binary=True, stop_words=event_detector.CZECH_STOPWORDS)\n",
    "    X = vectorizer.fit_transform(cluster_docs)\n",
    "    inverse = {v: k for k, v in vectorizer.vocabulary_.items()}    \n",
    "    detect_events(X, cluster_days, cluster_stream_len, inverse, cluster)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
