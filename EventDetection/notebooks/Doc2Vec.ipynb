{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2Vec test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import math\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "from importlib import reload\n",
    "from pprint import pprint\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "import gensim\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from event_detection import data_fetchers, event_detector, preprocessing\n",
    "\n",
    "reload(logging)\n",
    "logging.basicConfig(stream=sys.stdout, format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fetcher = data_fetchers.CzechFullTexts(dataset='dec-jan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "    def __init__(self, documents):\n",
    "        self.documents = documents\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for doc in self.documents:\n",
    "            yield gensim.utils.simple_preprocess(doc.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word2vec_path = '../event_detection/gensim/word2vec'\n",
    "documents = Preprocessor(fetcher)\n",
    "\n",
    "if os.path.exists(word2vec_path):\n",
    "    word2vec_model = gensim.models.Word2Vec.load(word2vec_path)\n",
    "else:\n",
    "    %time word2vec_model = gensim.models.Word2Vec(documents, size=100, negative=5, hs=0, min_count=2, window=5, iter=5)\n",
    "    word2vec_model.save(word2vec_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('charlie')\n",
    "pprint(word2vec_model.most_similar('charlie', topn=10))\n",
    "\n",
    "print('terorista')\n",
    "pprint(word2vec_model.most_similar('terorista', topn=10))\n",
    "\n",
    "print('vánoce')\n",
    "pprint(word2vec_model.most_similar('vánoce', topn=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Direct clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "clusters = defaultdict(list)\n",
    "\n",
    "for i, word_vec in enumerate(word2vec_model.syn0):\n",
    "    clusters[np.argmax(word_vec)].append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_clusters = len(clusters)\n",
    "print('Clusters:', n_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i, cluster in clusters.items():\n",
    "    few_indices = np.random.randint(low=0, high=len(cluster), size=min(10, len(cluster)))\n",
    "    word_indices = [cluster[ix] for ix in few_indices]\n",
    "    \n",
    "    print('----- {} -----'.format(i))\n",
    "    \n",
    "    for word_ix in word_indices:\n",
    "        print(word2vec_model.index2word[word_ix])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_clusters = 15\n",
    "word_vectors = normalize(word2vec_model.syn0, norm='l2', copy=True)\n",
    "\n",
    "clusterer = MiniBatchKMeans(n_clusters=n_clusters, n_init=10, random_state=1)\n",
    "%time labels = clusterer.fit_predict(word_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clusters = [[] for _ in range(n_clusters)]\n",
    "\n",
    "for word_ix, label in np.ndenumerate(labels):\n",
    "    clusters[label].append(word_ix[0])\n",
    "\n",
    "for i, c in enumerate(clusters):\n",
    "    print('Cluster {} of {} words'.format(i, len(c)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i, cluster in enumerate(clusters):\n",
    "    few_indices = np.random.randint(low=0, high=len(cluster), size=min(10, len(cluster)))\n",
    "    word_indices = [cluster[ix] for ix in few_indices]\n",
    "    \n",
    "    print('----- {} -----'.format(i))\n",
    "    \n",
    "    for word_ix in word_indices:\n",
    "        print(word2vec_model.index2word[word_ix])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copied from the project to play around with different tags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DocumentTagger:\n",
    "    def __init__(self, documents):\n",
    "        self.documents = documents\n",
    "        self.splitter = re.compile(r'\\W+')\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for i, doc in enumerate(self.documents):\n",
    "            tags = [doc.date, doc.category]\n",
    "            words = self.splitter.split(doc.text.lower())\n",
    "            tagged_doc = gensim.models.doc2vec.TaggedDocument(words, tags)\n",
    "\n",
    "            yield tagged_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fetcher = data_fetchers.CzechFullTexts(dataset='dec-jan', names=True, dates=True)\n",
    "doc_tagger = DocumentTagger(fetcher)\n",
    "logging.info('Document iterators prepared')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc2vec_path = '../event_detection/gensim/doc2vec'\n",
    "\n",
    "if os.path.exists(doc2vec_path):\n",
    "    doc2vec_model = gensim.models.Doc2Vec.load(doc2vec_path)\n",
    "else:\n",
    "    %time doc2vec_model = gensim.models.Doc2Vec(doc_tagger, dm=1, dm_mean=1, size=100, negative=5, hs=0, min_count=2, window=5, iter=5)\n",
    "    doc2vec_model.save(doc2vec_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "documents = list(fetcher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc_id = np.random.randint(doc2vec_model.docvecs.count)\n",
    "\n",
    "\n",
    "print('----- DOCUMENT -----')\n",
    "print(doc_id)\n",
    "print(documents[doc_id].name)\n",
    "print(documents[doc_id].text)\n",
    "\n",
    "sims = doc2vec_model.docvecs.most_similar(doc_id, topn=1)\n",
    "\n",
    "print('----- MOST SIMILAR -----')\n",
    "print(sims)\n",
    "\n",
    "print(documents[sims[0][0]].name)\n",
    "print(documents[sims[0][0]].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "* Finds clickbaits well (10 things you have never heard about! -- tend to be similar)\n",
    "* The document set contains a lot of duplicated articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "* Compare these:\n",
    "    1. classical event detection\n",
    "    2. event detection with pre-clustering\n",
    "    3. clustering-based event detection\n",
    "    4. clustering-based event detection with pre-clustering\n",
    "    5. doc2vec-similarity-based event detection\n",
    "    6. doc2vec + pre-clustering\n",
    "    7. doc2vec + cluster-based\n",
    "    8. doc2vec + pre-clustering + cluster-based\n",
    "* Try different doc2vec settings (concat, DBOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc_id = np.random.randint(doc2vec_model.docvecs.count)\n",
    "\n",
    "print(doc_id)\n",
    "print(documents[doc_id].name)\n",
    "\n",
    "sims = doc2vec_model.docvecs.most_similar(doc_id, topn=len(documents))\n",
    "\n",
    "print('-' * 10 + ' MOST SIMILAR ' + '-' * 10)\n",
    "for sim in sims[:10]:\n",
    "    print(documents[sim[0]].name, '\\t', sim)\n",
    "\n",
    "print('-' * 10 + ' LEAST SIMILAR ' + '-' * 10)\n",
    "for sim in sims[-10:]:\n",
    "    print(documents[sim[0]].name, '\\t', sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Trained\n",
    "document_vectors = doc2vec_model.docvecs[[i for i in range(len(documents))]]\n",
    "normalize(document_vectors, norm='l2', copy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_clusters = 15\n",
    "\n",
    "clusterer = MiniBatchKMeans(n_clusters=n_clusters, n_init=10, random_state=1)\n",
    "%time labels = clusterer.fit_predict(document_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clusters = [[] for _ in range(n_clusters)]\n",
    "\n",
    "for document_ix, label in np.ndenumerate(labels):\n",
    "    clusters[label].append(document_ix[0])\n",
    "\n",
    "for i, c in enumerate(clusters):\n",
    "    print('Cluster {} of {} documents'.format(i, len(c)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i, cluster in enumerate(clusters):\n",
    "    few_indices = np.random.randint(low=0, high=len(cluster), size=min(10, len(cluster)))\n",
    "    doc_indices = [cluster[ix] for ix in few_indices]\n",
    "    \n",
    "    print('----- {} -----'.format(i))\n",
    "    \n",
    "    for doc_ix in doc_indices:\n",
    "        print(documents[doc_ix].date, documents[doc_ix].name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster.k_means_ import (\n",
    "    _init_centroids,\n",
    "    _labels_inertia,\n",
    "    _tolerance,\n",
    "    _validate_center_shape,\n",
    ")\n",
    "from sklearn.utils import (\n",
    "    check_array,\n",
    "    check_random_state,\n",
    "    as_float_array,\n",
    ")\n",
    "from sklearn.cluster import _k_means\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.externals.joblib import Parallel, delayed\n",
    "from sklearn.utils.extmath import row_norms, squared_norm\n",
    "\n",
    "\n",
    "def _spherical_kmeans_single_lloyd(X, n_clusters, max_iter=300,\n",
    "                                   init='k-means++', verbose=False,\n",
    "                                   x_squared_norms=None,\n",
    "                                   random_state=None, tol=1e-4,\n",
    "                                   precompute_distances=True):\n",
    "    '''\n",
    "    Modified from sklearn.cluster.k_means_.k_means_single_lloyd.\n",
    "    '''\n",
    "    random_state = check_random_state(random_state)\n",
    "\n",
    "    best_labels, best_inertia, best_centers = None, None, None\n",
    "\n",
    "    # init\n",
    "    centers = _init_centroids(X, n_clusters, init, random_state=random_state,\n",
    "                              x_squared_norms=x_squared_norms)\n",
    "    if verbose:\n",
    "        print(\"Initialization complete\")\n",
    "\n",
    "    # Allocate memory to store the distances for each sample to its\n",
    "    # closer center for reallocation in case of ties\n",
    "    distances = np.zeros(shape=(X.shape[0],), dtype=X.dtype)\n",
    "\n",
    "    # iterations\n",
    "    for i in range(max_iter):\n",
    "        centers_old = centers.copy()\n",
    "\n",
    "        # labels assignment\n",
    "        # TODO: _labels_inertia should be done with cosine distance\n",
    "        #       since ||a - b|| = 2(1 - cos(a,b)) when a,b are unit normalized\n",
    "        #       this doesn't really matter.\n",
    "        labels, inertia = \\\n",
    "            _labels_inertia(X, x_squared_norms, centers,\n",
    "                            precompute_distances=precompute_distances,\n",
    "                            distances=distances)\n",
    "\n",
    "        # computation of the means\n",
    "        if sp.issparse(X):\n",
    "            centers = _k_means._centers_sparse(X, labels, n_clusters,\n",
    "                                               distances)\n",
    "        else:\n",
    "            centers = _k_means._centers_dense(X, labels, n_clusters, distances)\n",
    "\n",
    "        # l2-normalize centers (this is the main contibution here)\n",
    "        centers = normalize(centers)\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Iteration %2d, inertia %.3f\" % (i, inertia))\n",
    "\n",
    "        if best_inertia is None or inertia < best_inertia:\n",
    "            best_labels = labels.copy()\n",
    "            best_centers = centers.copy()\n",
    "            best_inertia = inertia\n",
    "\n",
    "        center_shift_total = squared_norm(centers_old - centers)\n",
    "        if center_shift_total <= tol:\n",
    "            if verbose:\n",
    "                print(\"Converged at iteration %d: \"\n",
    "                      \"center shift %e within tolerance %e\"\n",
    "                      % (i, center_shift_total, tol))\n",
    "            break\n",
    "\n",
    "    if center_shift_total > 0:\n",
    "        # rerun E-step in case of non-convergence so that predicted labels\n",
    "        # match cluster centers\n",
    "        best_labels, best_inertia = \\\n",
    "            _labels_inertia(X, x_squared_norms, best_centers,\n",
    "                            precompute_distances=precompute_distances,\n",
    "                            distances=distances)\n",
    "\n",
    "    return best_labels, best_inertia, best_centers, i + 1\n",
    "\n",
    "\n",
    "def spherical_k_means(X, n_clusters, init='k-means++', n_init=10,\n",
    "            max_iter=300, verbose=False, tol=1e-4, random_state=None,\n",
    "            copy_x=True, n_jobs=1, algorithm=\"auto\", return_n_iter=False):\n",
    "    \"\"\"Modified from sklearn.cluster.k_means_.k_means.\n",
    "    \"\"\"\n",
    "    if n_init <= 0:\n",
    "        raise ValueError(\"Invalid number of initializations.\"\n",
    "                         \" n_init=%d must be bigger than zero.\" % n_init)\n",
    "    random_state = check_random_state(random_state)\n",
    "\n",
    "    if max_iter <= 0:\n",
    "        raise ValueError('Number of iterations should be a positive number,'\n",
    "                         ' got %d instead' % max_iter)\n",
    "\n",
    "    best_inertia = np.infty\n",
    "    X = as_float_array(X, copy=copy_x)\n",
    "    tol = _tolerance(X, tol)\n",
    "\n",
    "    if hasattr(init, '__array__'):\n",
    "        init = check_array(init, dtype=X.dtype.type, copy=True)\n",
    "        _validate_center_shape(X, n_clusters, init)\n",
    "\n",
    "        if n_init != 1:\n",
    "            warnings.warn(\n",
    "                'Explicit initial center position passed: '\n",
    "                'performing only one init in k-means instead of n_init=%d'\n",
    "                % n_init, RuntimeWarning, stacklevel=2)\n",
    "            n_init = 1\n",
    "\n",
    "    # precompute squared norms of data points\n",
    "    x_squared_norms = row_norms(X, squared=True)\n",
    "\n",
    "    if n_jobs == 1:\n",
    "        # For a single thread, less memory is needed if we just store one set\n",
    "        # of the best results (as opposed to one set per run per thread).\n",
    "        for it in range(n_init):\n",
    "            # run a k-means once\n",
    "            labels, inertia, centers, n_iter_ = _spherical_kmeans_single_lloyd(\n",
    "                X, n_clusters, max_iter=max_iter, init=init, verbose=verbose,\n",
    "                tol=tol, x_squared_norms=x_squared_norms,\n",
    "                random_state=random_state)\n",
    "\n",
    "            # determine if these results are the best so far\n",
    "            if best_inertia is None or inertia < best_inertia:\n",
    "                best_labels = labels.copy()\n",
    "                best_centers = centers.copy()\n",
    "                best_inertia = inertia\n",
    "                best_n_iter = n_iter_\n",
    "    else:\n",
    "        # parallelisation of k-means runs\n",
    "        seeds = random_state.randint(np.iinfo(np.int32).max, size=n_init)\n",
    "        results = Parallel(n_jobs=n_jobs, verbose=0)(\n",
    "            delayed(_spherical_kmeans_single_lloyd)(X, n_clusters,\n",
    "                                   max_iter=max_iter, init=init,\n",
    "                                   verbose=verbose, tol=tol,\n",
    "                                   x_squared_norms=x_squared_norms,\n",
    "                                   # Change seed to ensure variety\n",
    "                                   random_state=seed)\n",
    "            for seed in seeds)\n",
    "\n",
    "        # Get results with the lowest inertia\n",
    "        labels, inertia, centers, n_iters = zip(*results)\n",
    "        best = np.argmin(inertia)\n",
    "        best_labels = labels[best]\n",
    "        best_inertia = inertia[best]\n",
    "        best_centers = centers[best]\n",
    "        best_n_iter = n_iters[best]\n",
    "\n",
    "    if return_n_iter:\n",
    "        return best_centers, best_labels, best_inertia, best_n_iter\n",
    "    else:\n",
    "        return best_centers, best_labels, best_inertia\n",
    "\n",
    "\n",
    "class SphericalKMeans(KMeans):\n",
    "    \"\"\"Spherical K-Means clustering\n",
    "    Modfication of sklearn.cluster.KMeans where cluster centers are normalized\n",
    "    (projected onto the sphere) in each iteration.\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_clusters : int, optional, default: 8\n",
    "        The number of clusters to form as well as the number of\n",
    "        centroids to generate.\n",
    "    max_iter : int, default: 300\n",
    "        Maximum number of iterations of the k-means algorithm for a\n",
    "        single run.\n",
    "    n_init : int, default: 10\n",
    "        Number of time the k-means algorithm will be run with different\n",
    "        centroid seeds. The final results will be the best output of\n",
    "        n_init consecutive runs in terms of inertia.\n",
    "    init : {'k-means++', 'random' or an ndarray}\n",
    "        Method for initialization, defaults to 'k-means++':\n",
    "        'k-means++' : selects initial cluster centers for k-mean\n",
    "        clustering in a smart way to speed up convergence. See section\n",
    "        Notes in k_init for more details.\n",
    "        'random': choose k observations (rows) at random from data for\n",
    "        the initial centroids.\n",
    "        If an ndarray is passed, it should be of shape (n_clusters, n_features)\n",
    "        and gives the initial centers.\n",
    "    tol : float, default: 1e-4\n",
    "        Relative tolerance with regards to inertia to declare convergence\n",
    "    n_jobs : int\n",
    "        The number of jobs to use for the computation. This works by computing\n",
    "        each of the n_init runs in parallel.\n",
    "        If -1 all CPUs are used. If 1 is given, no parallel computing code is\n",
    "        used at all, which is useful for debugging. For n_jobs below -1,\n",
    "        (n_cpus + 1 + n_jobs) are used. Thus for n_jobs = -2, all CPUs but one\n",
    "        are used.\n",
    "    random_state : integer or numpy.RandomState, optional\n",
    "        The generator used to initialize the centers. If an integer is\n",
    "        given, it fixes the seed. Defaults to the global numpy random\n",
    "        number generator.\n",
    "    verbose : int, default 0\n",
    "        Verbosity mode.\n",
    "    copy_x : boolean, default True\n",
    "        When pre-computing distances it is more numerically accurate to center\n",
    "        the data first.  If copy_x is True, then the original data is not\n",
    "        modified.  If False, the original data is modified, and put back before\n",
    "        the function returns, but small numerical differences may be introduced\n",
    "        by subtracting and then adding the data mean.\n",
    "    Attributes\n",
    "    ----------\n",
    "    cluster_centers_ : array, [n_clusters, n_features]\n",
    "        Coordinates of cluster centers\n",
    "    labels_ :\n",
    "        Labels of each point\n",
    "    inertia_ : float\n",
    "        Sum of distances of samples to their closest cluster center.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_clusters=8, init='k-means++', n_init=10,\n",
    "                 max_iter=300, tol=1e-4, n_jobs=1,\n",
    "                 verbose=0, random_state=None, copy_x=True):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.init = init\n",
    "        self.max_iter = max_iter\n",
    "        self.tol = tol\n",
    "        self.n_init = n_init\n",
    "        self.verbose = verbose\n",
    "        self.random_state = random_state\n",
    "        self.copy_x = copy_x\n",
    "        self.n_jobs = n_jobs\n",
    "\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"Compute k-means clustering.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like or sparse matrix, shape=(n_samples, n_features)\n",
    "        \"\"\"\n",
    "        random_state = check_random_state(self.random_state)\n",
    "        X = self._check_fit_data(X)\n",
    "\n",
    "        # TODO: add check that all data is unit-normalized\n",
    "\n",
    "        self.cluster_centers_, self.labels_, self.inertia_, self.n_iter_ = \\\n",
    "            spherical_k_means(\n",
    "                X, n_clusters=self.n_clusters, init=self.init,\n",
    "                n_init=self.n_init, max_iter=self.max_iter, verbose=self.verbose,\n",
    "                tol=self.tol, random_state=random_state, copy_x=self.copy_x,\n",
    "                n_jobs=self.n_jobs,\n",
    "                return_n_iter=True)\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_sphere_clusters = 15\n",
    "\n",
    "sphere_clusterer = SphericalKMeans(n_clusters=n_clusters, n_init=10, random_state=1)\n",
    "%time sphere_labels = sphere_clusterer.fit_predict(document_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sphere_clusters = [[] for _ in range(n_sphere_clusters)]\n",
    "\n",
    "for document_ix, label in np.ndenumerate(sphere_labels):\n",
    "    sphere_clusters[label].append(document_ix[0])\n",
    "\n",
    "for i, c in enumerate(sphere_clusters):\n",
    "    print('Cluster {} of {} documents'.format(i, len(c)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i, cluster in enumerate(sphere_clusters):\n",
    "    few_indices = np.random.randint(low=0, high=len(cluster), size=10)\n",
    "    doc_indices = [cluster[ix] for ix in few_indices]\n",
    "    \n",
    "    print('----- {} -----'.format(i))\n",
    "    \n",
    "    for doc_ix in doc_indices:\n",
    "        print(documents[doc_ix].name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering of features by trajectory (useless)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t = time()\n",
    "documents, relative_days = data_fetchers.fetch_czech_corpus_dec_jan()\n",
    "\n",
    "stream_length = max(relative_days) + 1  # Zero-based, hence the + 1.\n",
    "logging.info('Read input in %fs.', time() - t)\n",
    "logging.info('Stream length: %d', stream_length)\n",
    "\n",
    "t = time()\n",
    "vectorizer = CountVectorizer(min_df=30, max_df=0.9, binary=True, stop_words=event_detector.CZECH_STOPWORDS)\n",
    "bow_matrix = vectorizer.fit_transform(documents)\n",
    "id2word = {v: k for k, v in vectorizer.vocabulary_.items()}\n",
    "logging.info('Done in %fs.', time() - t)\n",
    "logging.info('BOW: %s, %s, storing %d elements', str(bow_matrix.shape), str(bow_matrix.dtype),\n",
    "                     bow_matrix.getnnz())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trajectories = event_detector.construct_feature_trajectories(bow_matrix, relative_days)\n",
    "dps, dp = event_detector.spectral_analysis(trajectories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_, n_days = trajectories.shape\n",
    "DPS_BOUNDARY = 0.03\n",
    "\n",
    "aperiodic_feature_indices = np.where((dps > DPS_BOUNDARY) & (dp > math.floor(n_days / 2)))[0]\n",
    "periodic_feature_indices = np.where((dps > DPS_BOUNDARY) & (dp <= math.floor(n_days / 2)))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aperiodic_trajectories = trajectories[aperiodic_feature_indices]\n",
    "periodic_trajectories = trajectories[periodic_feature_indices]\n",
    "logging.info('Aperiodic trajectories: %s', str(aperiodic_trajectories.shape))\n",
    "logging.info('Periodic trajectories: %s', str(periodic_trajectories.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "normalize(aperiodic_trajectories, norm='l1', copy=False)\n",
    "normalize(periodic_trajectories, norm='l1', copy=False)\n",
    "\n",
    "aperiodic_pairwise = np.zeros((aperiodic_trajectories.shape[0], aperiodic_trajectories.shape[0]), dtype=float)\n",
    "periodic_pairwise = np.zeros((periodic_trajectories.shape[0], periodic_trajectories.shape[0]), dtype=float)\n",
    "\n",
    "for i in range(len(aperiodic_pairwise)):\n",
    "    for j in range(len(aperiodic_pairwise)):\n",
    "        aperiodic_pairwise[i, j] = event_detector.jensen_shannon_divergence(aperiodic_trajectories[i], aperiodic_trajectories[j])\n",
    "        \n",
    "for i in range(len(periodic_pairwise)):\n",
    "    for j in range(len(periodic_pairwise)):\n",
    "        periodic_pairwise[i, j] = event_detector.jensen_shannon_divergence(periodic_trajectories[i], periodic_trajectories[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import AffinityPropagation, KMeans\n",
    "\n",
    "# aperiodic_clusterer = KMeans(n_clusters=10, n_init=10) #DBSCAN(metric='precomputed', algorithm='auto')\n",
    "# periodic_clusterer = KMeans(n_clusters=10, n_init=10) #DBSCAN(metric='precomputed', algorithm='auto')\n",
    "\n",
    "aperiodic_clusterer = DBSCAN(metric='precomputed')\n",
    "periodic_clusterer = DBSCAN(metric='precomputed')\n",
    "\n",
    "aperiodic_labels = aperiodic_clusterer.fit_predict(np.sqrt(aperiodic_pairwise))\n",
    "periodic_labels = periodic_clusterer.fit_predict(np.sqrt(periodic_pairwise))\n",
    "\n",
    "aperiodic_n_clusters = len(set(aperiodic_labels)) - (1 if -1 in aperiodic_labels else 0)\n",
    "periodic_n_clusters = len(set(periodic_labels)) - (1 if -1 in periodic_labels else 0)\n",
    "\n",
    "logging.info('Aperiodic clusters: %d', aperiodic_n_clusters)\n",
    "logging.info('Periodic clusters: %d', periodic_n_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aperiodic_clusters = [[] for _ in range(aperiodic_n_clusters)]\n",
    "\n",
    "for feature_ix, label in np.ndenumerate(aperiodic_labels):\n",
    "    aperiodic_clusters[label].append(feature_ix[0])\n",
    "\n",
    "periodic_clusters = [[] for _ in range(periodic_n_clusters)]\n",
    "\n",
    "for feature_ix, label in np.ndenumerate(periodic_labels):\n",
    "    periodic_clusters[label].append(feature_ix[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for cluster in aperiodic_clusters:\n",
    "    for word in cluster:\n",
    "        trajectory = aperiodic_trajectories[word]\n",
    "        plt.plot(trajectory)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for cluster in periodic_clusters:\n",
    "    for word in cluster:\n",
    "        trajectory = periodic_trajectories[word]\n",
    "        plt.plot(trajectory)\n",
    "        \n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}