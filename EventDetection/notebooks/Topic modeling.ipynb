{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "from imp import reload\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.stats import cauchy, norm\n",
    "\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "from sklearn.decomposition import LatentDirichletAllocation, NMF, TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import normalize, Normalizer\n",
    "import sklearn.mixture as gmm\n",
    "\n",
    "import gensim\n",
    "import wordcloud\n",
    "\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from event_detection import data_fetchers, event_detector\n",
    "\n",
    "reload(logging)\n",
    "logging.basicConfig(stream=sys.stdout, format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Fetch data\n",
    "(only the Dec-Jan subset for now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total_time = time()\n",
    "\n",
    "t = time()\n",
    "documents, relative_days = data_fetchers.fetch_czech_corpus_dec_jan()\n",
    "\n",
    "stream_length = max(relative_days) + 1  # Zero-based, hence the + 1.\n",
    "print('Read input in %fs.' % (time() - t))\n",
    "print('Stream length: %d' % stream_length)\n",
    "\n",
    "t = time()\n",
    "vectorizer = CountVectorizer(min_df=30, max_df=100000, binary=True, stop_words=event_detector.CZECH_STOPWORDS)\n",
    "bow_matrix = vectorizer.fit_transform(documents).tocsr()\n",
    "id2word = {v: k for k, v in vectorizer.vocabulary_.items()}\n",
    "print('Created bag of words in %fs.' % (time() - t))\n",
    "print('BOW:', bow_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Detect events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trajectories = event_detector.construct_feature_trajectories(bow_matrix, relative_days)\n",
    "dps, dp = event_detector.spectral_analysis(trajectories)\n",
    "\n",
    "# Aperiodic events\n",
    "aperiodic_indices = np.where((dps > event_detector.DPS_BOUNDARY) & (dp > math.ceil(stream_length / 2)))[0]\n",
    "aperiodic_bow = bow_matrix[:, aperiodic_indices]\n",
    "aperiodic_features = trajectories[aperiodic_indices, :]\n",
    "aperiodic_dps = dps[aperiodic_indices]\n",
    "aperiodic_dp = dp[aperiodic_indices]\n",
    "aperiodic_events = event_detector.unsupervised_greedy_event_detection(aperiodic_indices, aperiodic_bow, aperiodic_features,\n",
    "                                                       aperiodic_dps, aperiodic_dp)\n",
    "\n",
    "# Periodic events\n",
    "periodic_indices = np.where((dps > event_detector.DPS_BOUNDARY) & (dp <= math.ceil(stream_length / 2)))[0]\n",
    "periodic_bow = bow_matrix[:, periodic_indices]\n",
    "periodic_features = trajectories[periodic_indices, :]\n",
    "periodic_dps = dps[periodic_indices]\n",
    "periodic_dp = dp[periodic_indices]\n",
    "periodic_events = event_detector.unsupervised_greedy_event_detection(periodic_indices, periodic_bow, periodic_features,\n",
    "                                                      periodic_dps, periodic_dp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generators to lists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "aperiodic_events = list(aperiodic_events)\n",
    "periodic_events = list(periodic_events)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Perform TFIDF rescaling\n",
    "Actually, just IDF rescaling, since the documents are preprocessed to contain each word just once. Nevertheless, this means a *huge* improvement. All methods generally perform better, some even faster, and there are even some new document types uncovered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%time bow_matrix = TfidfTransformer().fit_transform(bow_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_TOPICS = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Gensim models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "corpus = gensim.matutils.Sparse2Corpus(bow_matrix, documents_columns=False)\n",
    "dictionary = gensim.corpora.Dictionary.from_corpus(corpus, id2word=id2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSI model\n",
    "Fitting takes about 1 minute for the Dec-Jan dataset with 10 topics and about 4 minutes for 100 topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "LSI_PATH = ('./dec_jan_%d_topics_tfidf.lsi' % NUM_TOPICS)\n",
    "\n",
    "if os.path.exists(LSI_PATH):\n",
    "    lsi = gensim.models.LsiModel.load(LSI_PATH)\n",
    "    print('Loaded %d LSI topics from file' % NUM_TOPICS)\n",
    "else:\n",
    "    %time lsi = gensim.models.LsiModel(corpus, id2word=dictionary, num_topics=NUM_TOPICS, onepass=False, power_iters=5)\n",
    "    lsi.save(LSI_PATH)\n",
    "    print('Generated LSI model for %d topics and saved to file' % NUM_TOPICS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Project the documents into the latent space (taken from Gensim GitHub FAQ) and normalize to unit l2 norm, which causes KMeans to behave as spherical KMeans:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "lsi_gensim = gensim.matutils.corpus2dense(lsi[corpus], len(lsi.projection.s)).T / lsi.projection.s\n",
    "normalize(lsi_gensim, norm='l2', copy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lsi.print_topics(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA model\n",
    "Fitting takes about 15 minutes for the Dec-Jan dataset with 10 topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "LDA_PATH = ('./dec_jan_%d_topics.lda' % NUM_TOPICS)\n",
    "\n",
    "if os.path.exists(LDA_PATH):\n",
    "    lda = gensim.models.LdaModel.load(LDA_PATH)\n",
    "    print('Loaded %d LDA topics from file' % NUM_TOPICS)\n",
    "else:\n",
    "    %time lda = gensim.models.LdaModel(corpus, id2word=dictionary, num_topics=NUM_TOPICS)\n",
    "    lda.save(LDA_PATH)\n",
    "    print('Generated LDA model for %d topics and saved to file' % NUM_TOPICS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda.print_topics(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import pyLDAvis\n",
    "# import pyLDAvis.gensim\n",
    "\n",
    "# pyLDAvis.enable_notebook()\n",
    "# pyLDAvis.gensim.prepare(lda, corpus, dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create indexed corpus\n",
    "Need to create an indexed corpus, otherwise corpus2csc is REALLY slow. The MM corpus itself took a long time to compute, and while the resulting clusters are a bit better, they are not good enough to justify the insane waiting time.\n",
    "\n",
    "This is an error - we use sparse matrix to represent dense data. When fixed, this step may actually be faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LDA_CORPUS_PATH = './lda_corpus.mm'\n",
    "\n",
    "if os.path.exists(LDA_CORPUS_PATH):\n",
    "    lda_corpus = gensim.corpora.MmCorpus('./lda_corpus.mm')\n",
    "else:\n",
    "    lda_corpus = lda[corpus]\n",
    "    gensim.corpora.MmCorpus.serialize('./lda_corpus.mm', lda_corpus)\n",
    "\n",
    "sparse_corpus = gensim.matutils.corpus2csc(lda_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Scikit-learn models\n",
    "Sklearn's LDA model is even slower than the Gensim one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSI model\n",
    "Fitting takes only about 12 seconds (using randomized SVD) and 8 seconds (using ARPACK SVD) for the Dec-Jan dataset with 10 topics. Normalization to unit l2 norm causes KMeans to behave as spherical KMeans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "svd = TruncatedSVD(n_components=NUM_TOPICS, algorithm='randomized')\n",
    "normalizer = Normalizer(norm='l2', copy=False)\n",
    "lsi_sklearn_model = make_pipeline(svd, normalizer)\n",
    "\n",
    "%time lsi_sklearn = lsi_sklearn_model.fit_transform(bow_matrix)\n",
    "print(lsi_sklearn.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NMF model\n",
    "Takes about 4 minutes for the Dec-Jan dataset with 10 topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nmf_model = NMF(n_components=NUM_TOPICS)\n",
    "%time nmf = nmf_model.fit_transform(bow_matrix)\n",
    "print(nmf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Document clustering\n",
    "Times for full KMeans (without TFIDF) and 10 topics and clusters:\n",
    "* Gensim LSI: about 40 seconds\n",
    "* Gensim LDA: about 4 minutes\n",
    "* Scikit-learn LSI: about 1 minute\n",
    "* NMF: about 30 seconds\n",
    "\n",
    "Times for MiniBatchKMeans are at most 2 seconds. The resulting clusters have only minor differences (this agrees with the documentation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "NUM_CLUSTERS = 15\n",
    "method = 'lsi_gensim'\n",
    "# kmeans = KMeans(n_clusters=NUM_CLUSTERS, random_state=1)\n",
    "kmeans = MiniBatchKMeans(n_clusters=NUM_CLUSTERS, n_init=10, random_state=1)\n",
    "\n",
    "if method == 'lsi_gensim':\n",
    "    model = lsi_gensim\n",
    "elif method == 'lda_gensim':\n",
    "    model = sparse_corpus\n",
    "elif method == 'lsi_sklearn':\n",
    "    model = lsi_sklearn\n",
    "elif method == 'nmf_sklearn':\n",
    "    model = nmf\n",
    "else:\n",
    "    print('Invalid method')\n",
    "\n",
    "%time kmeans.fit(model)\n",
    "\n",
    "clusters = [[] for _ in range(NUM_CLUSTERS)]\n",
    "\n",
    "for doc, label in np.ndenumerate(kmeans.labels_):\n",
    "    clusters[label].append(doc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cluster_words = [[] for _ in range(NUM_CLUSTERS)]\n",
    "\n",
    "for i, cluster in enumerate(clusters):\n",
    "    for doc in cluster:\n",
    "        cluster_words[i].extend(documents[doc].split(r'\\s'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i, words in enumerate(cluster_words):\n",
    "    wc = wordcloud.WordCloud(width=2000, height=1200).generate(' '.join(words))\n",
    "    fig = plt.figure()\n",
    "    fig.set_size_inches(20, 12)\n",
    "    plt.imshow(wc)\n",
    "    plt.axis('off')\n",
    "    wc.to_file('./sklearn_lsi_15_topics_15_clusters/%02d.png' % i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Get event trajectories and bursty periods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aperiodic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gaussian_curve(value, loc, scale):\n",
    "    return norm.pdf(value, loc=loc, scale=scale)\n",
    "\n",
    "aperiodic_parameters = []  # List of triples (event_id, event_mean, event_std).\n",
    "\n",
    "for i, event in enumerate(aperiodic_events):\n",
    "    # Create trajectory.\n",
    "    e_trajectory, e_dominant_period = event_detector.create_event_trajectory(event, trajectories, dps, dp)\n",
    "    \n",
    "    # Calculate moving average & cutoff.\n",
    "    ma = event_detector.moving_average(e_trajectory, event_detector.WINDOW)\n",
    "    ma_mean = np.mean(ma)\n",
    "    ma_std = np.std(ma)\n",
    "    cutoff = ma_mean + ma_std\n",
    "    \n",
    "    # Fit Gaussian curve.\n",
    "    n_days = len(e_trajectory)\n",
    "    days = np.arange(n_days)\n",
    "    \n",
    "    peak_indices = np.where(e_trajectory > cutoff)\n",
    "    peak_days = peak_indices[0]\n",
    "    peaks = e_trajectory[peak_indices].reshape(-1)\n",
    "    peaks /= np.sum(peaks)  # Normalize the DFIDF so it can be interpreted as probability.\n",
    "\n",
    "    p0 = (peak_days[len(peak_days) // 2], len(peak_days) / 4)\n",
    "    popt, pcov = curve_fit(gaussian_curve, peak_days, peaks, p0=p0, bounds=(0.0, n_days))\n",
    "    \n",
    "    # Extract parameters.\n",
    "    mean, std = popt\n",
    "    aperiodic_parameters.append((i, mean, std))\n",
    "    \n",
    "    # Plot some graphs.\n",
    "    if i % 10 == 0:\n",
    "        x = np.linspace(0.0, n_days, 1000)\n",
    "        plt.title('Bursty period: (%d, %d)' % (math.floor(mean - std), math.ceil(mean + std)))\n",
    "        plt.xlim(0.0, n_days)\n",
    "        plt.hlines(cutoff / np.sum(e_trajectory), 0, n_days, colors='g', linestyles='dashed', linewidth=1.5)\n",
    "        plt.axvline(math.floor(mean - std), color='r')\n",
    "        plt.axvline(math.ceil(mean + std), color='r')\n",
    "        plt.plot(days, e_trajectory / np.sum(e_trajectory))\n",
    "        plt.plot(x, norm.pdf(x, mean, std))\n",
    "        plt.show()\n",
    "\n",
    "print('Found %d aperiodic events' % len(aperiodic_parameters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Periodic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "periodic_parameters = []  # List of pairs (event_id, [(event_loc, event_scale)]).\n",
    "\n",
    "for i, event in enumerate(periodic_events):\n",
    "    # Create trajectory.\n",
    "    e_trajectory, e_dominant_period = event_detector.create_event_trajectory(event, trajectories, dps, dp)\n",
    "\n",
    "    # Calculate moving average & cutoff.\n",
    "    ma = event_detector.moving_average(e_trajectory, event_detector.WINDOW)\n",
    "    ma_mean = np.mean(ma)\n",
    "    ma_std = np.std(ma)\n",
    "    cutoff = ma_mean + ma_std\n",
    "\n",
    "    n_days = len(e_trajectory)\n",
    "    days = np.arange(n_days)\n",
    "\n",
    "    observations = np.hstack((days.reshape(-1, 1), e_trajectory.reshape(-1, 1)))\n",
    "    observations = observations[observations[:, 1] > cutoff, :]\n",
    "    normalized_trajectory = e_trajectory / np.sum(e_trajectory)\n",
    "\n",
    "    # Fit mixture model.\n",
    "    n_components = int(min(np.floor(n_days / e_dominant_period), len(observations)))\n",
    "    g = gmm.GaussianMixture(n_components=n_components, covariance_type='diag')\n",
    "    g.fit(observations)\n",
    "\n",
    "    e_parameters = []\n",
    "    \n",
    "    # Extract parameters.\n",
    "    for mean_, cov_ in zip(g.means_, g.covariances_):\n",
    "        loc = mean_[0]\n",
    "        hwhm = np.sqrt(2 * np.log(2)) * np.sqrt(cov_[0])\n",
    "        \n",
    "        e_parameters.append((loc, hwhm))\n",
    "\n",
    "    periodic_parameters.append((i, e_parameters))\n",
    "    \n",
    "    # Plot some graphs.\n",
    "    if i % 10 == 0:\n",
    "        x = np.linspace(0.0, n_days, 1000)\n",
    "        components = np.squeeze(np.array(\n",
    "            [cauchy.pdf(x, mean[0], np.sqrt(2 * np.log(2)) * np.sqrt(cov[0])) for mean, cov in zip(g.means_, g.covariances_)]))\n",
    "        \n",
    "        pdf = g.weights_ @ components\n",
    "\n",
    "        plt.title('DP: %d, n_components: %d' % (e_dominant_period, n_components))\n",
    "        plt.xlim(0.0, n_days)\n",
    "        plt.hlines(cutoff / np.sum(e_trajectory), 0, n_days, colors='g', linestyles='dashed', linewidth=1.5)\n",
    "        plt.plot(days, e_trajectory / np.sum(e_trajectory))\n",
    "        plt.plot(x, pdf)\n",
    "        plt.show()\n",
    "\n",
    "print('Found %d periodic events' % len(periodic_parameters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Get event documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aperiodic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_samples, n_features = bow_matrix.shape\n",
    "dtd = event_detector.construct_doc_to_day_matrix(n_samples, relative_days)\n",
    "aperiodic_documents = []  # List of pairs (event_id, array_of_event_document_ids).\n",
    "\n",
    "for event, event_parameters in zip(aperiodic_events, aperiodic_parameters):\n",
    "    event_id, event_mean, event_std = event_parameters\n",
    "\n",
    "    burst_start = max(math.floor(event_mean - event_std), 0)  # If an event burst starts right at day 0, this would get negative.\n",
    "    burst_end = min(math.ceil(event_mean + event_std), stream_length - 1)  # If an event burst ends at stream length, this would exceed the boundary.\n",
    "\n",
    "    # Documents published on burst days.\n",
    "    docs_dates, _ = dtd[:, burst_start:burst_end + 1].nonzero()  # There is exactly one '1' in every row.\n",
    "\n",
    "    # Documents containing at least one of the event word features.\n",
    "    docs_either_words = bow_matrix[:, event]\n",
    "\n",
    "    # Documents containing all of the event word features.\n",
    "    docs_words = np.where(docs_either_words.getnnz(axis=1) == len(event))[0]\n",
    "\n",
    "    # Documents both published on burst days and containing all event word features.\n",
    "    docs_both = np.intersect1d(docs_dates, docs_words, assume_unique=True)\n",
    "    \n",
    "    aperiodic_documents.append((event_id, docs_both))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Periodic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_samples, n_features = bow_matrix.shape\n",
    "dtd = event_detector.construct_doc_to_day_matrix(n_samples, relative_days)\n",
    "periodic_documents = []  # List of pairs (event_id, array_of_event_document_ids).\n",
    "\n",
    "for event, event_parameters in zip(periodic_events, periodic_parameters):\n",
    "    event_id, locs_n_scales = event_parameters\n",
    "    \n",
    "    docs_dates = []\n",
    "\n",
    "    for loc, scale in locs_n_scales:\n",
    "        burst_start = max(math.floor(loc - scale), 0)\n",
    "        burst_end = min(math.ceil(loc + scale), stream_length - 1)\n",
    "        \n",
    "        burst_dates, _ = dtd[:, burst_start:burst_end + 1].nonzero()\n",
    "        docs_dates.extend(burst_dates.tolist())\n",
    "    \n",
    "    # Documents containing at least one of the event word features.\n",
    "    docs_either_words = bow_matrix[:, event]\n",
    "\n",
    "    # Documents containing all of the event word features.\n",
    "    docs_words = np.where(docs_either_words.getnnz(axis=1) == len(event))[0]\n",
    "    \n",
    "    # Documents both published on burst days and containing all event word features.\n",
    "    # Do not assume unique, as some bursty periods may overlap and fetch the same document twice.\n",
    "    docs_both = np.intersect1d(docs_dates, docs_words, assume_unique=False)\n",
    "    \n",
    "    periodic_documents.append((event_id, docs_both))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Event clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Greedy approach\n",
    "Simply select the document set of every event, assign each document to a cluster and put the event to the cluster where majority of the documents belongs. This assumes that the documents are semantically similar enough to more or less agree on a cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aperiodic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aperiodic_event_clusters1 = [[] for _ in range(NUM_CLUSTERS)]\n",
    "\n",
    "for event_id, event_docs in aperiodic_documents:\n",
    "    if len(event_docs) == 0:\n",
    "        print('Skipped event %d due to having empty document set' % event_id)\n",
    "        continue\n",
    "    \n",
    "    docs_to_clusters = kmeans.predict(model[event_docs, :])\n",
    "\n",
    "    mean = np.mean(docs_to_clusters)\n",
    "    std = np.std(docs_to_clusters)\n",
    "    mode = np.bincount(docs_to_clusters).argmax()\n",
    "    median = np.median(docs_to_clusters)\n",
    "\n",
    "    # Display document-cluster statistics.\n",
    "    print('Event %2d: (mean: %.2f, std: %.2f, mode: %d, median: %d)' % (event_id, mean, std, mode, median))\n",
    "    aperiodic_event_clusters1[mode].append(event_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i, event_cluster in enumerate(aperiodic_event_clusters1):\n",
    "    print('Cluster %d' % i)\n",
    "\n",
    "    for j, event in enumerate(event_cluster):\n",
    "        words = [id2word[word_id] for word_id in aperiodic_events[event]]\n",
    "        print('%02d. Event %2d (A): [%s]' % (j + 1, event, ', '.join(words)))\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Periodic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "periodic_event_clusters1 = [[] for _ in range(NUM_CLUSTERS)]\n",
    "\n",
    "for event_id, event_docs in periodic_documents:\n",
    "    if len(event_docs) == 0:\n",
    "        print('Skipped event %d due to having empty document set' % event_id)\n",
    "        continue\n",
    "\n",
    "    docs_to_clusters = kmeans.predict(model[event_docs, :])\n",
    "    \n",
    "    mean = np.mean(docs_to_clusters)\n",
    "    std = np.std(docs_to_clusters)\n",
    "    mode = np.bincount(docs_to_clusters).argmax()\n",
    "    median = np.median(docs_to_clusters)\n",
    "    \n",
    "    # Display document-cluster statistics.\n",
    "    print('Event %2d: (mean: %.2f, std: %.2f, mode: %d, median: %d)' % (event_id, mean, std, mode, median))\n",
    "    periodic_event_clusters1[mode].append(event_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i, event_cluster in enumerate(periodic_event_clusters1):\n",
    "    print('Cluster %d' % i)\n",
    "\n",
    "    for j, event in enumerate(event_cluster):\n",
    "        words = [id2word[word_id] for word_id in periodic_events[event]]\n",
    "        print('%02d. Event %2d (P): [%s]' % (j + 1, event, ', '.join(words)))\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('./greedy.txt', 'w') as f:\n",
    "    for i, (aperiodic_cluster, periodic_cluster) in enumerate(zip(aperiodic_event_clusters1, periodic_event_clusters1)):\n",
    "        print('Cluster %d' % i, file=f)\n",
    "\n",
    "        j = 0\n",
    "\n",
    "        for aperiodic_event in aperiodic_cluster:\n",
    "            words = [id2word[word_id] for word_id in aperiodic_events[aperiodic_event]]\n",
    "            print('%02d. Event %2d (A): [%s]' % (j + 1, aperiodic_event, ', '.join(words)), file=f)\n",
    "            j += 1\n",
    "\n",
    "        for periodic_event in periodic_cluster:\n",
    "            words = [id2word[word_id] for word_id in periodic_events[periodic_event]]\n",
    "            print('%02d. Event %2d (P): [%s]' % (j + 1, periodic_event, ', '.join(words)), file=f)\n",
    "            j += 1\n",
    "\n",
    "        print(file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector averaging\n",
    "Compute an average vector from  all event documents and predict its cluster. Only minor differences when compared to the greedy approach, but it seems more robust."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aperiodic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aperiodic_event_clusters2 = [[] for _ in range(NUM_CLUSTERS)]\n",
    "\n",
    "for event_id, event_docs in aperiodic_documents:\n",
    "    if len(event_docs) == 0:\n",
    "        print('Skipped event %d due to having empty document set' % event_id)\n",
    "        continue\n",
    "\n",
    "    mean_doc = np.mean(model[event_docs, :], axis=0)\n",
    "    mean_doc /= np.linalg.norm(mean_doc)\n",
    "    cluster_id = kmeans.predict(mean_doc.reshape(1, -1))\n",
    "    aperiodic_event_clusters2[cluster_id].append(event_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i, event_cluster in enumerate(aperiodic_event_clusters2):\n",
    "    print('Cluster %d' % i)\n",
    "\n",
    "    for j, event in enumerate(event_cluster):\n",
    "        words = [id2word[word_id] for word_id in aperiodic_events[event]]\n",
    "        print('%02d. Event %2d (A): [%s]' % (j + 1, event, ', '.join(words)))\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Periodic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "periodic_event_clusters2 = [[] for _ in range(NUM_CLUSTERS)]\n",
    "\n",
    "for event_id, event_docs in periodic_documents:\n",
    "    if len(event_docs) == 0:\n",
    "        print('Skipped event %d due to having empty document set' % event_id)\n",
    "        continue\n",
    "\n",
    "    mean_doc = np.mean(model[event_docs, :], axis=0)\n",
    "    mean_doc /= np.linalg.norm(mean_doc)\n",
    "    cluster_id = kmeans.predict(mean_doc.reshape(1, -1))\n",
    "    periodic_event_clusters2[cluster_id].append(event_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i, event_cluster in enumerate(periodic_event_clusters2):\n",
    "    print('Cluster %d' % i)\n",
    "\n",
    "    for j, event in enumerate(event_cluster):\n",
    "        words = [id2word[word_id] for word_id in periodic_events[event]]\n",
    "        print('%02d. Event %2d (P): [%s]' % (j + 1, event, ', '.join(words)))\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('./average.txt', 'w') as f:\n",
    "    for i, (aperiodic_cluster, periodic_cluster) in enumerate(zip(aperiodic_event_clusters2, periodic_event_clusters2)):\n",
    "        print('Cluster %d' % i, file=f)\n",
    "\n",
    "        j = 0\n",
    "\n",
    "        for aperiodic_event in aperiodic_cluster:\n",
    "            words = [id2word[word_id] for word_id in aperiodic_events[aperiodic_event]]\n",
    "            print('%02d. Event %2d (A): [%s]' % (j + 1, aperiodic_event, ', '.join(words)), file=f)\n",
    "            j += 1\n",
    "\n",
    "        for periodic_event in periodic_cluster:\n",
    "            words = [id2word[word_id] for word_id in periodic_events[periodic_event]]\n",
    "            print('%02d. Event %2d (P): [%s]' % (j + 1, periodic_event, ', '.join(words)), file=f)\n",
    "            j += 1\n",
    "\n",
    "        print(file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Hierarchical clustering test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = lsi_gensim\n",
    "aperiodic_means = []\n",
    "event_titles = []\n",
    "\n",
    "for event_id, event_docs in aperiodic_documents:\n",
    "    if len(event_docs) == 0:\n",
    "        print('Skipped event %d due to having empty document set' % event_id)\n",
    "        print([id2word[word_id] for word_id in aperiodic_events[event_id]])\n",
    "        continue\n",
    "\n",
    "    mean_doc = np.mean(model[event_docs, :], axis=0)\n",
    "    aperiodic_means.append(mean_doc)\n",
    "    \n",
    "    event = aperiodic_events[event_id]\n",
    "    event_words = []\n",
    "    \n",
    "    for word_id in event:\n",
    "        event_words.append(id2word[word_id])\n",
    "        \n",
    "    event_titles.append(', '.join(event_words))\n",
    "    \n",
    "aperiodic_means = np.array(aperiodic_means, dtype=float)\n",
    "print(aperiodic_means.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import linkage\n",
    "%time linkage_matrix = linkage(aperiodic_means, method='weighted', metric='cosine')\n",
    "print(linkage_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 20)) # set size\n",
    "ax = dendrogram(linkage_matrix, orientation=\"right\", labels=event_titles);\n",
    "\n",
    "plt.tick_params(\\\n",
    "    axis= 'x',          # changes apply to the x-axis\n",
    "    which='both',      # both major and minor ticks are affected\n",
    "    bottom='off',      # ticks along the bottom edge are off\n",
    "    top='off',         # ticks along the top edge are off\n",
    "    labelbottom='off')\n",
    "\n",
    "plt.tight_layout() #show plot with tight layout\n",
    "plt.savefig('cosine_weighted_clusters.png', dpi=200)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
