{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import math\n",
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import sys\n",
    "from importlib import reload\n",
    "from pprint import pprint\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.stats import cauchy, norm\n",
    "\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import sklearn.mixture as gmm\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "import gensim\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from event_detection import data_fetchers, event_detector, plotting, preprocessing\n",
    "\n",
    "reload(logging)\n",
    "logging.basicConfig(stream=sys.stdout, format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fetcher = data_fetchers.CzechFullTexts(dataset='dec-jan', names=True, dates=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "    def __init__(self, documents):\n",
    "        self.documents = documents\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for doc in self.documents:\n",
    "            yield gensim.utils.simple_preprocess(doc.text)\n",
    "\n",
    "\n",
    "word2vec_path = '../event_detection/gensim/word2vec'\n",
    "documents = Preprocessor(fetcher)\n",
    "\n",
    "if os.path.exists(word2vec_path):\n",
    "    print('Loading Word2Vec')\n",
    "    %time word2vec_model = gensim.models.Word2Vec.load(word2vec_path)\n",
    "else:\n",
    "    print('Training Word2Vec')\n",
    "    %time word2vec_model = gensim.models.Word2Vec(documents, size=100, negative=5, hs=0, min_count=2, window=5, iter=5)\n",
    "    word2vec_model.save(word2vec_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DocumentTagger:\n",
    "    def __init__(self, documents):\n",
    "        self.documents = documents\n",
    "        \n",
    "    def __iter__(self):\n",
    "        for i, doc in enumerate(self.documents):\n",
    "            tags = [i]\n",
    "            words = gensim.utils.simple_preprocess(doc.text)\n",
    "            tagged_doc = gensim.models.doc2vec.TaggedDocument(words, tags)\n",
    "\n",
    "            yield tagged_doc\n",
    "\n",
    "\n",
    "doc2vec_path = '../event_detection/gensim/doc2vec'\n",
    "doc_tagger = DocumentTagger(fetcher)\n",
    "\n",
    "if os.path.exists(doc2vec_path):\n",
    "    print('Loading Doc2Vec')\n",
    "    %time doc2vec_model = gensim.models.Doc2Vec.load(doc2vec_path)\n",
    "else:\n",
    "    print('Training Doc2Vec')\n",
    "    %time doc2vec_model = gensim.models.Doc2Vec(doc_tagger, dm=1, dm_mean=1, size=100, negative=5, hs=0, min_count=2, window=5, iter=5)\n",
    "    doc2vec_model.save(doc2vec_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare event detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "PICKLE_PATH = '../event_detection/pickle'\n",
    "ID2WORD_PATH = os.path.join(PICKLE_PATH, 'vectorizer_dec_jan_full_nolimit.pickle')\n",
    "BOW_MATRIX_PATH = os.path.join(PICKLE_PATH, 'term_document_dec_jan_full_nolimit.pickle')\n",
    "RELATIVE_DAYS_PATH = os.path.join(PICKLE_PATH, 'relative_days_dec_jan_full_nolimit.pickle')\n",
    "\n",
    "if os.path.exists(ID2WORD_PATH) and os.path.exists(BOW_MATRIX_PATH) and os.path.exists(RELATIVE_DAYS_PATH):\n",
    "    with open(ID2WORD_PATH, mode='rb') as f:\n",
    "        id2word = pickle.load(f)\n",
    "\n",
    "    with open(BOW_MATRIX_PATH, mode='rb') as f:\n",
    "        bow_matrix = pickle.load(f)\n",
    "\n",
    "    with open(RELATIVE_DAYS_PATH, mode='rb') as f:\n",
    "        relative_days = pickle.load(f)\n",
    "\n",
    "    stream_length = max(relative_days) + 1  # Zero-based, hence the + 1.\n",
    "\n",
    "    logging.info('Deserialized id2word, bag of words matrix and relative days')\n",
    "    logging.info('BOW: %s, %s, storing %d elements', str(bow_matrix.shape), str(bow_matrix.dtype),\n",
    "                 bow_matrix.getnnz())\n",
    "    logging.info('Stream length: %d', stream_length)\n",
    "else:\n",
    "    if not os.path.exists(PICKLE_PATH):\n",
    "        os.makedirs(PICKLE_PATH)\n",
    "\n",
    "    t = time()\n",
    "    relative_days = fetcher.fetch_relative_days()\n",
    "\n",
    "    stream_length = max(relative_days) + 1  # Zero-based, hence the + 1.\n",
    "    logging.info('Read input in %fs.', time() - t)\n",
    "    logging.info('Stream length: %d', stream_length)\n",
    "    \n",
    "    documents = Preprocessor(fetcher)\n",
    "\n",
    "    t = time()\n",
    "    vectorizer = CountVectorizer(min_df=2, binary=True, tokenizer=lambda doc: doc, preprocessor=lambda doc: doc)\n",
    "    bow_matrix = vectorizer.fit_transform(documents)\n",
    "    id2word = {v: k for k, v in vectorizer.vocabulary_.items()}\n",
    "\n",
    "    with open(ID2WORD_PATH, mode='wb') as f:\n",
    "        pickle.dump(id2word, f)\n",
    "\n",
    "    with open(BOW_MATRIX_PATH, mode='wb') as f:\n",
    "        pickle.dump(bow_matrix, f)\n",
    "\n",
    "    with open(RELATIVE_DAYS_PATH, mode='wb') as f:\n",
    "        pickle.dump(relative_days, f)\n",
    "\n",
    "    logging.info('Created and serialized id2word, bag of words matrix and relative days in %fs.', time() - t)\n",
    "    logging.info('BOW: %s, %s, storing %d elements', str(bow_matrix.shape), str(bow_matrix.dtype),\n",
    "                 bow_matrix.getnnz())\n",
    "\n",
    "trajectories = event_detector.construct_feature_trajectories(bow_matrix, relative_days)\n",
    "dps, dp = event_detector.spectral_analysis(trajectories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc2vec_model.syn0.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Individual event detectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Vanilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unsupervised_greedy_event_detection(global_indices, bow_matrix, feature_trajectories, dps, dp):\n",
    "    def cost_function(feature_indices):\n",
    "        with np.errstate(divide='ignore'):  # Denominator == 0 means no document overlap and return infinity.\n",
    "            return event_detector.set_similarity(feature_indices, divergences) / (\n",
    "                event_detector.set_df_overlap(feature_indices, overlaps) * np.sum(dps[feature_indices]))\n",
    "\n",
    "    def minimizing_feature(event_so_far, feature_indices):\n",
    "        index = feature_indices[0]\n",
    "        min_cost = cost_function(event_so_far + [feature_indices[0]])\n",
    "\n",
    "        for f in feature_indices[1:]:\n",
    "            added = event_so_far + [f]\n",
    "            added_cost = cost_function(added)\n",
    "\n",
    "            if added_cost < min_cost:\n",
    "                index, min_cost = f, added_cost\n",
    "\n",
    "        return index, min_cost\n",
    "\n",
    "    logging.info('Examining %d features.', len(feature_trajectories))\n",
    "\n",
    "    # Sort feature indices by DPS in ascending order.\n",
    "    indices = list(sorted(range(len(feature_trajectories)), key=lambda i: dps[i]))\n",
    "\n",
    "    t = time()\n",
    "    divergences = event_detector.precompute_divergences(feature_trajectories, dp)\n",
    "    logging.info('Precomputed information divergences in %fs.', time() - t)\n",
    "\n",
    "    t = time()\n",
    "    overlaps = event_detector.precompute_df_overlaps(bow_matrix)\n",
    "    logging.info('Precomputed document overlaps in %fs.', time() - t)\n",
    "\n",
    "    t = time()\n",
    "    found_events = 0\n",
    "\n",
    "    while len(indices) > 0:\n",
    "        feature = indices.pop(0)\n",
    "        event = [feature]\n",
    "        event_cost = 1 / dps[feature]\n",
    "\n",
    "        while len(indices) > 0:\n",
    "            m, cost = minimizing_feature(event, indices)\n",
    "\n",
    "            if cost < event_cost:\n",
    "                event.append(m)\n",
    "                indices.remove(m)\n",
    "                event_cost = cost\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        yield global_indices[event]\n",
    "        found_events += 1\n",
    "\n",
    "    logging.info('Detected %d events in %fs.', found_events, time() - t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Vanilla, cluster-based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unsupervised_greedy_event_detection_clusters(global_indices, bow_matrix, feature_trajectories, dps, dp):\n",
    "    logging.info('Examining %d features.', len(feature_trajectories))\n",
    "\n",
    "    t0 = time()\n",
    "    t = time()\n",
    "    divergences = event_detector.precompute_divergences(feature_trajectories, dp)\n",
    "    logging.info('Precomputed information divergences in %fs.', time() - t)\n",
    "\n",
    "    t = time()\n",
    "    overlaps = event_detector.precompute_df_overlaps(bow_matrix).A.astype(float)\n",
    "    overlaps[overlaps < 1e-8] = 1e-12\n",
    "    logging.info('Precomputed document overlaps in %fs.', time() - t)\n",
    "\n",
    "    np.divide(divergences, overlaps, out=divergences)\n",
    "    del overlaps\n",
    "\n",
    "    logging.info('Created similarity matrix in %fs.', time() - t0)\n",
    "\n",
    "    from sklearn.cluster import DBSCAN\n",
    "\n",
    "    t = time()\n",
    "    clusterer = DBSCAN(metric='precomputed', min_samples=2, eps=0.25)\n",
    "    divergences[np.isinf(divergences)] = 1e15\n",
    "\n",
    "    labels = clusterer.fit_predict(divergences)\n",
    "    logging.info('Performed clustering in %fs.', time() - t)\n",
    "    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "\n",
    "    if n_clusters == 0:  # TODO: Temp fix.\n",
    "        logging.warning('Found 0 clusters.')\n",
    "        return []\n",
    "\n",
    "    logging.info('Found %d clusters.', n_clusters)\n",
    "    events = [[] for _ in range(n_clusters)]\n",
    "\n",
    "    for feature_ix, label in np.ndenumerate(labels):\n",
    "        if label >= 0:\n",
    "            events[label].append(global_indices[feature_ix[0]])\n",
    "\n",
    "    logging.info('Detected %d events in %fs.', len(events), time() - t0)\n",
    "    logging.info('Total features covered: %d.', sum(len(event) for event in events))\n",
    "\n",
    "    yield from events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import entropy\n",
    "\n",
    "def event_detection_word2vec(global_indices, bow_matrix, feature_trajectories, dps, dp, id2word):\n",
    "    def cost_function(old_indices, new_index):\n",
    "        with np.errstate(divide='ignore'):  # Denominator == 0 means no document overlap and return infinity.\n",
    "            old_traj = np.mean(feature_trajectories[old_indices], axis=0)\n",
    "            new_traj = feature_trajectories[new_index]\n",
    "            traj_div = event_detector.jensen_shannon_divergence(old_traj, new_traj)\n",
    "            \n",
    "            old_words = [id2word[global_indices[word_ix]] for word_ix in old_indices]\n",
    "            new_word = id2word[global_indices[new_index]]\n",
    "            doc_sim = math.exp(word2vec_model.n_similarity(old_words, [new_word]))\n",
    "            \n",
    "            dps_score = np.sum(dps[old_indices + [new_index]])\n",
    "            \n",
    "            return traj_div / (doc_sim * dps_score)\n",
    "\n",
    "    def minimizing_feature(event_so_far, feature_indices):\n",
    "        index = feature_indices[0]\n",
    "        min_cost = cost_function(event_so_far, feature_indices[0])\n",
    "\n",
    "        for f in feature_indices[1:]:\n",
    "            added_cost = cost_function(event_so_far, f)\n",
    "\n",
    "            if added_cost < min_cost:\n",
    "                index, min_cost = f, added_cost\n",
    "\n",
    "        return index, min_cost\n",
    "\n",
    "    logging.info('Examining %d features.', len(feature_trajectories))\n",
    "\n",
    "    # Sort feature indices by DPS in ascending order.\n",
    "    indices = list(sorted(range(len(feature_trajectories)), key=lambda i: dps[i]))\n",
    "\n",
    "    t = time()\n",
    "    found_events = 0\n",
    "\n",
    "    while len(indices) > 0:\n",
    "        feature = indices.pop(0)\n",
    "        event = [feature]\n",
    "        event_cost = 1 / dps[feature]\n",
    "\n",
    "        while len(indices) > 0:\n",
    "            m, cost = minimizing_feature(event, indices)\n",
    "\n",
    "            if cost < event_cost:\n",
    "                event.append(m)\n",
    "                indices.remove(m)\n",
    "                event_cost = cost\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        yield global_indices[event]\n",
    "        found_events += 1\n",
    "\n",
    "    logging.info('Detected %d events in %fs.', found_events, time() - t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Word2Vec, cluster-based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def event_detection_word2vec_clusters(global_indices, bow_matrix, feature_trajectories, dps, dp, id2word):\n",
    "    logging.info('Examining %d features.', len(feature_trajectories))\n",
    "\n",
    "    t0 = time()\n",
    "    t = time()\n",
    "    divergences = event_detector.precompute_divergences(feature_trajectories, dp)\n",
    "    logging.info('Precomputed information divergences in %fs.', time() - t)\n",
    "    \n",
    "    overlaps = np.zeros((bow_matrix.shape[1], bow_matrix.shape[1]), dtype=float)\n",
    "    logging.info('Overlaps: %s', str(overlaps.shape))\n",
    "    \n",
    "    for i in range(len(overlaps)):\n",
    "        for j in range(len(overlaps)):\n",
    "            if i != j:\n",
    "                word1 = id2word[global_indices[i]]\n",
    "                word2 = id2word[global_indices[j]]\n",
    "                similarity = word2vec_model.similarity(word1, word2)\n",
    "                overlaps[i, j] = math.exp(similarity)\n",
    "            \n",
    "\n",
    "    logging.info('Precomputed word similarities in %fs.', time() - t)\n",
    "\n",
    "    np.divide(divergences, overlaps, out=divergences)\n",
    "    del overlaps\n",
    "    \n",
    "    divergences[np.isnan(divergences)] = 1000.0\n",
    "    \n",
    "    logging.info('Created similarity matrix in %fs.', time() - t0)\n",
    "\n",
    "    from sklearn.cluster import DBSCAN\n",
    "\n",
    "    t = time()\n",
    "    clusterer = DBSCAN(metric='precomputed', min_samples=2, eps=100.0)\n",
    "\n",
    "    labels = clusterer.fit_predict(divergences)\n",
    "    logging.info('Performed clustering in %fs.', time() - t)\n",
    "    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "\n",
    "    if n_clusters == 0:  # TODO: Temp fix.\n",
    "        logging.warning('Found 0 clusters.')\n",
    "        return []\n",
    "\n",
    "    logging.info('Found %d clusters.', n_clusters)\n",
    "    events = [[] for _ in range(n_clusters)]\n",
    "    \n",
    "    for feature_ix, label in np.ndenumerate(labels):\n",
    "        events[label].append(global_indices[feature_ix[0]])\n",
    "\n",
    "    logging.info('Detected %d events in %fs.', len(events), time() - t0)\n",
    "\n",
    "    yield from events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Word vectors from Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import entropy\n",
    "\n",
    "def event_detection_word_doc2vec(global_indices, bow_matrix, feature_trajectories, dps, dp, id2word):\n",
    "    def cost_function(old_indices, new_index):\n",
    "        with np.errstate(divide='ignore'):  # Denominator == 0 means no document overlap and return infinity.\n",
    "            old_traj = np.mean(feature_trajectories[old_indices], axis=0)\n",
    "            new_traj = feature_trajectories[new_index]\n",
    "            traj_div = event_detector.jensen_shannon_divergence(old_traj, new_traj)\n",
    "            \n",
    "            old_words = [id2word[global_indices[word_ix]] for word_ix in old_indices]\n",
    "            new_word = id2word[global_indices[new_index]]\n",
    "            doc_sim = math.exp(doc2vec_model.n_similarity(old_words, [new_word]))\n",
    "            \n",
    "            dps_score = np.sum(dps[old_indices + [new_index]])\n",
    "            \n",
    "            return traj_div / (doc_sim * dps_score)\n",
    "\n",
    "    def minimizing_feature(event_so_far, feature_indices):\n",
    "        index = feature_indices[0]\n",
    "        min_cost = cost_function(event_so_far, feature_indices[0])\n",
    "\n",
    "        for f in feature_indices[1:]:\n",
    "            added_cost = cost_function(event_so_far, f)\n",
    "\n",
    "            if added_cost < min_cost:\n",
    "                index, min_cost = f, added_cost\n",
    "\n",
    "        return index, min_cost\n",
    "\n",
    "    logging.info('Examining %d features.', len(feature_trajectories))\n",
    "\n",
    "    # Sort feature indices by DPS in ascending order.\n",
    "    indices = list(sorted(range(len(feature_trajectories)), key=lambda i: dps[i]))\n",
    "    \n",
    "    t = time()\n",
    "    found_events = 0\n",
    "\n",
    "    while len(indices) > 0:\n",
    "        feature = indices.pop(0)\n",
    "        event = [feature]\n",
    "        event_cost = 1 / dps[feature]\n",
    "\n",
    "        while len(indices) > 0:\n",
    "            m, cost = minimizing_feature(event, indices)\n",
    "\n",
    "            if cost < event_cost:\n",
    "                event.append(m)\n",
    "                indices.remove(m)\n",
    "                event_cost = cost\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        yield global_indices[event]\n",
    "        found_events += 1\n",
    "\n",
    "    logging.info('Detected %d events in %fs.', found_events, time() - t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Word vectors from Doc2Vec, cluster-based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def event_detection_word_doc2vec_clusters(global_indices, bow_matrix, feature_trajectories, dps, dp, id2word):\n",
    "    logging.info('Examining %d features.', len(feature_trajectories))\n",
    "\n",
    "    t0 = time()\n",
    "    t = time()\n",
    "    divergences = event_detector.precompute_divergences(feature_trajectories, dp)\n",
    "    logging.info('Precomputed information divergences in %fs.', time() - t)\n",
    "    \n",
    "    overlaps = np.zeros((bow_matrix.shape[1], bow_matrix.shape[1]), dtype=float)\n",
    "    logging.info('Overlaps: %s', str(overlaps.shape))\n",
    "    \n",
    "    for i in range(len(overlaps)):\n",
    "        for j in range(len(overlaps)):\n",
    "            if i != j:\n",
    "                word1 = id2word[global_indices[i]]\n",
    "                word2 = id2word[global_indices[j]]\n",
    "                similarity = doc2vec_model.similarity(word1, word2)\n",
    "                overlaps[i, j] = math.exp(similarity)\n",
    "                \n",
    "    logging.info('Precomputed word similarities in %fs.', time() - t)\n",
    "\n",
    "    np.divide(divergences, overlaps, out=divergences)\n",
    "    del overlaps\n",
    "    \n",
    "    divergences[np.isnan(divergences)] = 1e-3\n",
    "    \n",
    "    logging.info('Created similarity matrix in %fs.', time() - t0)\n",
    "\n",
    "    from hdbscan import HDBSCAN\n",
    "\n",
    "    t = time()\n",
    "    clusterer = HDBSCAN(metric='precomputed', min_samples=2, min_cluster_size=3)\n",
    "    \n",
    "    labels = clusterer.fit_predict(divergences)\n",
    "    logging.info('Performed clustering in %fs.', time() - t)\n",
    "    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "\n",
    "    if n_clusters == 0:  # TODO: Temp fix.\n",
    "        logging.warning('Found 0 clusters.')\n",
    "        return []\n",
    "\n",
    "    logging.info('Found %d clusters.', n_clusters)\n",
    "    events = [[] for _ in range(n_clusters)]\n",
    "    \n",
    "    for feature_ix, label in np.ndenumerate(labels):\n",
    "        if label >= 0:\n",
    "            events[label].append(global_indices[feature_ix[0]])\n",
    "\n",
    "    logging.info('Detected %d events in %fs.', len(events), time() - t0)\n",
    "    logging.info('Total features covered: %d.', sum(len(event) for event in events))\n",
    "\n",
    "    yield from events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "method = 'vanilla_clusters'\n",
    "\n",
    "if method == 'vanilla':\n",
    "    function = unsupervised_greedy_event_detection\n",
    "elif method == 'vanilla_clusters':\n",
    "    function = unsupervised_greedy_event_detection_clusters\n",
    "elif method == 'word2vec':\n",
    "    function = lambda global_indices, bow_matrix, feature_trajectories, dps, dp: \\\n",
    "        event_detection_word2vec(global_indices, bow_matrix, feature_trajectories, dps, dp, id2word)\n",
    "elif method == 'word2vec_clusters':\n",
    "    function = lambda global_indices, bow_matrix, feature_trajectories, dps, dp: \\\n",
    "        event_detection_word2vec_clusters(global_indices, bow_matrix, feature_trajectories, dps, dp, id2word)\n",
    "elif method == 'word_doc2vec':\n",
    "    function = lambda global_indices, bow_matrix, feature_trajectories, dps, dp: \\\n",
    "        event_detection_word_doc2vec(global_indices, bow_matrix, feature_trajectories, dps, dp, id2word)\n",
    "elif method == 'word_doc2vec_clusters':\n",
    "    function = lambda global_indices, bow_matrix, feature_trajectories, dps, dp: \\\n",
    "        event_detection_word_doc2vec_clusters(global_indices, bow_matrix, feature_trajectories, dps, dp, id2word)\n",
    "else:\n",
    "    raise ValueError('Unknown method')\n",
    "    \n",
    "DPS_BOUNDARY = 0.03\n",
    "\n",
    "def detect_events(bow_matrix, feature_trajectories, dps, dp, aperiodic):\n",
    "    _, n_days = feature_trajectories.shape\n",
    "\n",
    "    if aperiodic:\n",
    "        feature_indices = np.where((dps > DPS_BOUNDARY) & (dp > math.floor(n_days / 2)))[0]\n",
    "    else:\n",
    "        feature_indices = np.where((dps > DPS_BOUNDARY) & (dp <= math.floor(n_days / 2)))[0]\n",
    "\n",
    "    if len(feature_indices) == 0:\n",
    "        logging.warning('No features to detect events from.')\n",
    "        return []\n",
    "\n",
    "    logging.info('Detecting %s events from %d features.', 'aperiodic' if aperiodic else 'periodic', len(feature_indices))\n",
    "    \n",
    "    bow_slice = bow_matrix[:, feature_indices]\n",
    "    features = feature_trajectories[feature_indices, :]\n",
    "    feature_dps = dps[feature_indices]\n",
    "    feature_dp = dp[feature_indices]\n",
    "\n",
    "    return function(feature_indices, bow_slice, features, feature_dps, feature_dp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform event detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Aperiodic events\n",
    "aperiodic_events = detect_events(bow_matrix, trajectories, dps, dp, aperiodic=True)\n",
    "plotting.plot_events(trajectories, aperiodic_events, id2word, dps, dp, dirname=('./' + method + '_aperiodic'))\n",
    "logging.info('Aperiodic done')\n",
    "\n",
    "# Periodic events\n",
    "periodic_events = detect_events(bow_matrix, trajectories, dps, dp, aperiodic=False)\n",
    "plotting.plot_events(trajectories, periodic_events, id2word, dps, dp, dirname=('./' + method + '_periodic'))\n",
    "logging.info('Periodic done')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}