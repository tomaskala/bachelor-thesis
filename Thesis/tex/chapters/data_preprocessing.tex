The collection we work with comes directly from webscraping without any special structure. The documents consist only of headlines, bodies and publication days. Furthermore, there are some noisy patterns such as residual HTML entities, typos, words cut in the middle, etc. To make the most of the following methods, we preprocess the documents to remove as many of these errors as possible, and also to gain some additional information about the text.

We will first employ some NLP methods to gain insight into the data. Then, we will train a model to obtain word embeddings.

Since most machine learning methods rely on the data being represented numerically, usually in a vector space, it is necessary to obtain such representation from the basic words we work with. Preferably, these vectors should retain as much information as possible. There are many ways to do so --- a simple TFIDF representation \cite{information-retrieval} which represents the words by weighted counts of their appearance in the document collection. More complicated methods, such as Latent Semantic Indexing \cite{lsi} attempt to discover latent structure within words to also reveal topical relations between them. This idea is further pursued by probabilistic topical models, such as Latent Dirichlet Allocation \cite{lda}.

In this thesis, we use the \textit{word2vec} model introduced by Tomáš Mikolov \cite{distributed-representations, linguistic-regularities, word2vec}, which uses a shallow neural network to project the words from some vocabulary into a vector space. Vectors in this space have interesting semantical properties, such as vector arithmetics preserving semantic relations, or semantically related words forming clusters.

Later on, we will need to measure word similarity. This will come up several times in the process of event detection --- in the event detection itself, later when querying the document collection to obtain documents related to the detected events, and finally to compute human-readable summaries. The word2vec models is fit for all of these uses, as opposed to the other mentioned approaches, some of which are designed only to measure document similarity, or, on the other hand, do not support document similarity queries very well.


\section{Document collection}
The input to the algorithm is a collection of $\doccount$ news documents containing full text articles along with their publication days, headlines and, possibly, additional metadata. We assume no preprocessing was done prior to running the algorithm.

If we denote $t_{i}$ as the publication day of a document $d_{i}$, the collection can be understood as a stream $\left\{ (d_{1}, t_{1}), (d_{2}, t_{2}), \dots, (d_{\doccount}, t_{\doccount}) \right\}$ with $t_{i} \leq t_{j}$ for $i < j$. Furthermore, we define $\streamlen$ to be the length of the stream (in days), and we normalize the document publication days to be relative to the document stream start; that is $t_{1} = 1$ and $t_{\doccount} = \streamlen$.


\section{Dataset}
{\color{red} TODO: Describe the dataset.}


\section{Preprocessing}
Some of the documents contain residual HTML entities from errors during web scraping, which we filter out using a manually constructed stopwords list.

We used the MorphoDiTa tagger \cite{morphodita} to perform tokenization, lemmatization and parts-of-speech tagging. Our whole analysis will be run on these lemmatized texts, and we will revert to the full forms only at the end when summarizing the events in a human-readable way.

{\color{red} TODO: Mikolov's phrase detection model?}


\section{Word embeddings} \label{word-embeddings}
Here, we will train the mentioned word2vec model. Although the training is time-consuming, the word vectors can be pre-trained on a large document collection and then reused in following runs, even on different documents.

For the training, we only discard punctuation marks and word denoted as unknown parts of speech my the tagger. Such unknown words are mostly typos or foreign language terms not important for our analysis. We do not remove any additional stopwords or parts of speech, as this would break word order, which is, to some extent, used by the word2vec model.

The thesis was implemented using the Gensim \cite{gensim} package. The project contains memory efficient, easy to use Python implementations of various topic modeling algorithms, word2vec included. In addition, we used the SciPy toolkit \cite{scipy} and Scikit-Learn \cite{scikit-learn} for various machine learning-related computations.

{\color{red} TODO: Describe the settings used in the algorithm (vector space dimensionality, avg/concat, etc.)}