\section{Document collection}
The input to the algorithm is a collection of $\doccount$ news documents containing full text articles along with their publication days, headlines and, possibly, additional metadata. We assume no preprocessing was done prior to running the algorithm.

If we denote $t_{i}$ as the publication day of a document $d_{i}$, the collection can be understood as a stream $\left\{ (d_{1}, t_{1}), (d_{2}, t_{2}), \dots, (d_{\doccount}, t_{\doccount}) \right\}$ with $t_{i} \leq t_{j}$ for $i < j$. Furthermore, we define $\streamlen$ to be the length of the stream (in days), and we normalize the document publication days to be relative to the document stream start; that is $t_{1} = 1$ and $t_{\doccount} = \streamlen$.


\section{Dataset}
{\color{red} TODO: Describe the dataset.}


\section{Preprocessing}
Some of the documents contain residual HTML entities from errors during web scraping, which we filter out using a manually constructed stopwords list.

We used the MorphoDiTa tagger \cite{morphodita} to perform tokenization, lemmatization and parts-of-speech tagging. Our whole analysis will be run on these lemmatized texts, and we will revert to the full forms only at the end when summarizing the events in a human-readable way.

{\color{red} TODO: Mikolov's phrase detection model?}


\section{Word embeddings} \label{word-embeddings}
Before we proceed to the event detection itself, we represent the word features as vectors by embedding them in a vector space. This representation will then be used to compare semantic similarity of words in a hope to obtain clusters of words concerning similar topics.

The method used is the \textit{word2vec} model introduced by Tomáš Mikolov \cite{distributed-representations, linguistic-regularities, word2vec}. We will use this model during the initial event detection phase, later to obtain document representation of the discovered events, and ultimately to obtain human-readable annotations.

The only additional preprocessing step apart from those described earlier is to discard punctuation marks from the words. We do not filter any additional stopwords, as this would break word order necessary for successful training of the word2vec model.

The thesis was implemented using the Gensim \cite{gensim} package. The project contains memory efficient, easy to use Python implementations of various topic modeling algorithms, word2vec included. In addition, we used the SciPy toolkit \cite{scipy} and Scikit-Learn \cite{scikit-learn} for various machine learning-related computations.

{\color{red} TODO: Describe the settings used in the algorithm (vector space dimensionality, avg/concat, etc.)}