\section{Document collection}
The input to the algorithm is a collection of news documents containing full text articles along with their publication days, headlines and, possibly, additional metadata. We assume no preprocessing was done prior to running the algorithm.

If we denote $t_{i}$ as the publication day of a document $d_{i}$, the collection can be understood as a stream $\left\{ (d_{1}, t_{1}), (d_{2}, t_{2}), \dots, (d_{\doccount}, t_{\doccount}) \right\}$ with $t_{i} \leq t_{j}$ for $i < j$. Furthermore, we define $\streamlen$ to be the length of the stream (in days), and we normalize the document publication days to be relative to the document stream start; that is $t_{1} = 1$ and $t_{\doccount} = \streamlen$.


\section{Preprocessing}
Some of the documents contain residual HTML entities from errors during web scraping, which we filter out using a manually constructed stopwords list.

No traditional stopwords filtering, e.g. keeping only Nouns, Verbs, Adjectives and Adverbs, is used, as this would break text continuity needed during word embeddings. Instead, our method chooses the important words based on their overall frequency and power of appearance in the collection.

The only additional preprocessing step is to convert all documents to lowercase, tokenize them by whitespace and strip punctuation marks.

{\color{red} TODO: Mikolov's phrase detection model?}


\section{Word embeddings} \label{word-embeddings}
Before we proceed to the event detection itself, we represent the word features as vectors by embedding them in a vector space. This representation will then be used to compare semantic similarity of words in a hope to obtain clusters of words concerning similar topics.

The method used is the \textit{word2vec} model \cite{word2vec} introduced by Tomas Mikolov. More specifically, we train its generalization, the \textit{doc2vec} model \cite{doc2vec} to obtain both document and word embeddings. The word vectors obtained will be used during the event detection phase, and the document vectors afterwards to examine the resulting events and extract annotations.

The thesis was implemented using the Gensim \cite{gensim} package. The project contains memory efficient, easy to use Python implementations of various topic modeling algorithms, word2vec included.

{\color{red} TODO: Describe the settings used in the algorithm (vector space dimensionality, avg/concat, etc.)}

{\color{red} TODO: Briefly describe how word2vec works.}