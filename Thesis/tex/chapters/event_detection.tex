%\begin{figure}[h]
%    \centering
%	\includegraphics[scale=0.5]{graph}
%	\caption{An example graph}
%	\label{fig:x sine graph}
%\end{figure}

To perform the actual event detection, we propose two algorithms. The first one is a modification of the greedy approach introduced by \cite{event-detection}, which forms events by greedily minimizing temporal and semantical distance of words. We build on top of this algorithm to incorporate the word embeddings as well as alter the temporal distance measure.

The second approach interprets events as literal clusters of words and uses a clustering algorithm to obtain them, rather than explicitly minimize the cost function.

In this chapter, we describe both algorithms, which we will later compare.


\section{Greedy approach}
This algorithm works by grouping word features together by greedily minimizing a cost function representing their temporal and semantic distance. We will first define a distance measure in each of these domains, combine them into the cost function and then describe the algorithm itself.

An event will be represented by a set of keywords. In each iteration, the keyword closest to the event built so far will be added to that event. We will therefore need to measure the distance between a word feature and a whole set of features.

\subsection{Measuring trajectory distance}
After normalization to sum up to 1, the trajectory $\vect{\traj}_{f}$ of a word feature $f$ can be interpreted as a probability distribution over days, with $\traj_{f}(i)$ denoting the probability that a random document published on day $i$ contains $f$. This interpretation allows us to compare the trajectories using information-theoretic techniques, notably the information divergence.

Given a set of word features $\featset$ and another feature $f$ with all trajectories normalized to probabilities, the temporal distance of $f$ to $\featset$ is

\begin{equation}
	\trajdist{\featset}{f} \coloneqq \kl{\vect{\bar{\traj}}_{\featset}}{\vect{\traj}_{f}},
\end{equation}

where $\vect{\bar{\traj}}_{\featset}$ is the mean of all trajectories of features in $\featset$, $\vect{\traj}_{f}$ is the trajectory of feature $f$ and $\kl{\cdot}{\cdot}$ denotes the Kullback-Leibler divergence.


\subsection{Measuring semantic similarity}
Most of the astounding results of the word2vec model arise from linguistic relations between words preserved under vector arithmetic \cite{linguistic-regularities}. Semantically similar words form clusters and are close to each other in terms of cosine similarity, which is a standard measure used in information retrieval \cite{information-retrieval, cosine-similarity}.

We take advantage of these properties obtain a more fine-grained measure of semantic similarity than \cite{event-detection}. The original method measured the similarity of two words only in terms of their document overlap -- number of documents both words both appear in, which eventually led to events represented by only a handful of keywords.

Given a set of word features $\featset$ and another feature $f \notin \featset$, the semantic similarity of $f$ and $\featset$ is

\begin{equation}
	\semsim{\featset}{f} \coloneqq \frac{\inp[\big]{\bar{\embed}_{\featset}}{\embed_{j}}}{\| \bar{\embed}_{\featset} \| \cdot \| \embed_{j} \|},
\end{equation}

where $\bar{\embed}_{\featset}$ is the mean of all vector embeddings of features in $\featset$ and $\embed_{f}$ is the vector embedding of the feature $f$. Here, the mean vector is supposed to represent a shared topic among words from $\featset$.


\subsection{Cost function}
Intuitively, an event should be represented by keywords highly correlated in the time domain, concerning the same topic, and with high enough power to be considered representative.

A cost function incorporating all these requirements is therefore defined as

\begin{equation} \label{eq:cost-function}
	\cost{\featset}{f} \coloneqq \frac{\trajdist{\featset}{f}}{\exp(\semsim{\featset}{f}) \cdot \sum_{g \in \featset \cup f}{\text{DPS}_{g}}},
\end{equation}

where we exponentiate the cosine similarity so that the resulting value is always positive. As the algorithm will minimize this function, the resulting events will have low trajectory divergence, high semantic similarity will consist of important keywords.


\subsection{Event detection}
To perform the event detection itself, we mostly adapt the \textit{Unsupervised greedy event detection} algorithm from \cite{event-detection}. We do make a change in the initial sorting and sort the word features in \textit{ascending} rather than descending order. This ensures that words with lower DPS value will get selected first and that the cost function will not be minimized as quickly. As a result, the events will contain more representative keywords. This effectively relaxes the DPS part of the cost function while keeping emphasis on the trajectory distance and semantic similarity.

\begin{algorithm}[H] \label{alg:greedy-event-detection}
\begin{algorithmic}[1]
\caption{Unsupervised greedy event detection}
\Input $\text{Word feature set} ~ F$

\State $\text{Sort the features in ascending DPS order: } DPS_{f_{1}} \leq \dots \leq DPS_{f_{\left\vert F \right\vert}}$

\State $k = 0$

\ForEach{$f \in F$}
	\State $k = k + 1$	
	\State $e_{k} = \{ f \}$
	\State $cost_{e_{k}} = \frac{1}{DPS_{f}}$
	\State $F = F \setminus f$
	
	\While{$F \neq \emptyset$}
		\State $m = \argmin\limits_{m}{\cost{e_{k}}{f_{m}}}$

		\If{$\cost{e_{k}}{f_{m}} < cost_{e_{k}}$}
			\State $cost_{e_{k}} = \cost{e_{k}}{f_{m}}$
			\State $e_{k} = e_{k} \cup f_{m}$
			\State $F = F \setminus f_{m}$
		\Else
			\Break
		\EndIf
	\EndWhile
\EndFor

\Output $\text{Events} ~ \{ e_{1}, e_{2}, \dots, e_{k} \}$
\end{algorithmic}
\end{algorithm}


\section{Cluster-based approach}
This approach uses a clustering algorithm to create clusters out of word features directly, instead of explicitly minimizing a cost function. To achieve this, we need to define a proper distance function, and choose the right clustering algorithm.

\subsection{Noise filtering}
Before we proceed to the clustering, we will filter out the noisy parts from the word trajectories. Most words are at some level reported all the time, though only a fraction of their trajectories belongs to some notable event. Unlike the greedy optimization described previously, clustering is prone to such noise, and would yield clusters of poor quality, often with trajectories being put together only due to their noisy parts being similar.

{\color{red} TODO: Graphs of awesome vs. poor clusterings}

We want to keep only those trajectory parts exceeding a certain frequency level, distinguishing notable bursts from the general noise. We do this by computing a cutoff value for each event trajectory and discarding the sectors falling under this cutoff. This procedure is adopted from \cite{online-search-queries}. The algorithm is as follows:

\begin{algorithm}[H]
\begin{algorithmic}[1]
\caption{Burst filtering}
\Input $\text{window-length} \ w,\ \text{event trajectory} \ \vect{\traj_{e}}$

\State $\vect{MA}_{w} = \text{Moving Average of length} ~ w ~ \text{for} ~ \vect{\traj}_{e} = \left[ \traj_{e}(1), \traj_{e}(2), \dots, \traj_{e}(\streamlen) \right]$

\State $\mathit{cutoff} = \text{mean} \left( \vect{MA}_{w} \right) + \text{std} \left( \vect{MA}_{w} \right)$

\State $\vect{bursts}_{e} = \left[ \traj_{e}(t) \mid \traj_{e}(t) > \mathit{cutoff} \right]$

\Output $\vect{bursts}_{e}$
\end{algorithmic}
\end{algorithm}

{\color{red} TODO: In the paper, they use $\vect{MA}(t)_{w} > \mathit{cutoff}$, try that. Also, why did I miss that? :(}


\subsection{Distance function}
The distance function conveys similar information as the cost function used in the previous algorithm, but it is defined pairwise between words.

Since the KL-divergence is not symmetric, we use the Jensen-Shannon divergence instead to measure trajectory distance. Here, we replace the cosine similarity by standard Euclidean distance to make the points spread farther apart, which helps to obtain better clustering.

The distance between two word features $f_{i}$,\ $f_{j}$ with trajectories $\vect{\traj}_{i},\ \vect{\traj}_{j}$ and embeddings $\embed_{i},\ \embed_{j}$ is therefore defined as

\begin{equation}
	\distfunc{f_{i}}{f_{j}} \coloneqq \jsd{\vect{\traj}_{i}}{\vect{\traj}_{j}} \cdot \| \embed_{i} - \embed_{j}\|_{2},
\end{equation}

with $\jsd{\vect{p}}{\vect{q}} = \frac{1}{2} \left( \kl{\vect{p}}{\vect{m}} + \kl{\vect{q}}{\vect{m}} \right) ,\ \vect{m} = \frac{1}{2} \left( \vect{p} + \vect{q} \right)$. As we replace the semantic ``similarity'' by ``distance'', we multiply the two components rather than divide them.


\subsection{Clustering algorithm}
We will need to select an appropriate clustering algorithm. The obvious requirement is that the algorithm must not depend on an a priori knowledge of the desired number of clusters. Another requirement is that the algorithm must accept custom distance measures. Speed is not the essence, since the number of representative words is typically in tens to hundreds.

Our experimentation suggests that the best results are obtained using the HDBSCAN algorithm \cite{hdbscan}.

{\color{red} TODO: Test this on the full dataset and if (H)DBSCAN proves functional, mention the noise detection.}

{\color{blue} TODO: Some comparison of the algorithms tested?}


\subsection{Event detection}
The input and output of the event detection algorithm remains the same as in \ref{alg:greedy-event-detection}, only the internals are different. That makes it easy to swap the two algorithms for comparison.

\begin{algorithm}[H]
\begin{algorithmic}[1]
\caption{Cluster-based event detection}
\Input $\text{Word feature set} ~ F$

\State Precompute a distance matrix $\distmat \in \R^{\left\vert F \right\vert \times \left\vert F \right\vert}$ with $\distmat_{ij} = \distfunc{f_{i}}{f_{j}}$

\State Apply HDBSCAN to $\distmat$, obtaining $k$ clusters and the noisy cluster

\ForEach{$(f, cluster) \in \text{HDBSCAN.clusters}$}
	\If{$cluster \neq noise$}
		\State $e_{cluster} = e_{cluster} \cup f$
	\EndIf
\EndFor

\Output $\text{Events} ~ \{ e_{1}, e_{2}, \dots, e_{k} \}$
\end{algorithmic}
\end{algorithm}
