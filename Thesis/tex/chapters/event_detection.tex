%\begin{figure}[h]
%    \centering
%	\includegraphics[scale=0.5]{graph}
%	\caption{An example graph}
%	\label{fig:x sine graph}
%\end{figure}

To perform the actual event detection, we propose two algorithms. The first one is a modification of the greedy approach introduced by \cite{event-detection}, which forms events by greedily minimizing temporal and semantical distance of words. We build on top of this algorithm to incorporate the word embeddings as well as alter the temporal distance measure.

The second approach interprets events as literal clusters of words and uses a clustering algorithm to obtain them, rather than explicit cost minimization.

In this section, we describe both algorithms, which we will later compare.


\section{Greedy approach}


\section{Cluster-based approach}


\section{Event detection}
To cluster the word features in both time and semantic domains, we will need to measure their distance. We first define a pairwise distance measure in each domain. These measures are then generalized to measure the distance of a feature from a whole set of features, so we can assign the closest feature to a partially constructed event. In the last step, these distances are combined into a single cost function which we aim to minimize.


\subsection{Measuring trajectory distance}

Distance between two feature trajectories is defined in terms of their information divergence. Unlike \cite{event-detection}, we use the \textit{Jensen-Shannon} divergence:

\begin{equation*}
	\featsim( \vect{p} \| \vect{q} ) = \frac{1}{2} \text{D}( \vect{p} \| \vect{m} ) + \frac{1}{2} \text{D}( \vect{q} \| \vect{m} ),
\end{equation*}

where $\vect{m} = \frac{1}{2} \left( \vect{p} + \vect{q} \right)$ and $\text{D}( \cdot \| \cdot )$ denotes the \textit{Kullback-Leibler} divergence.

Next, this measure is generalized to measure the distance between a feature set $\featset$ and another feature $f$ as

\begin{equation}
	\trajdist( \featset, f ) = \kl{\vect{\bar{\traj}}_{\featset}}{\vect{\traj}_{f}},
\end{equation}

where $\vect{\bar{\traj}}_{\featset}$ is the mean of all trajectories of features in $\featset$ and $\vect{\traj}_{f}$ is the trajectory of feature f.

{\color{red} TODO: Move the pairwise similarities to the cluster-based algorithm, keep only the set-feature ones here.}

{\color{blue} TODO: Try KL(mean(M),f) instead of JSD(mean(M),f)? Like M is the true distribution and f assumed. Keep JSD in pairwise similarity though, clustering likes symmetry.}


\subsection{Measuring semantic similarity}

The semantic similarity is again first defined for two features, and then generalized to a similarity between a feature set and another feature. Here, we utilize the word embeddings computed in \ref{word-embeddings}.

Most of the astounding results of the word2vec model come from semantic relations between words being preserved under vector arithmetic, with words concerning the same topic having roughly the same angle. Thus, we use the cosine similarity between embeddings $\embed_{i}$ and $\embed_{j}$ of word features $i$ and $j$:

\begin{equation}
	\semsim( f_{i}, f_{j} ) \coloneqq \frac{\inp[\big]{\embed_{i}}{\embed_{j}}}{\| \embed_{i} \| \cdot \| \embed_{j} \|}
\end{equation}

Next, we generalize the similarity for a feature set $\featset$ and another feature $f$ as

\begin{equation}
	\semsim( \featset, f ) \coloneqq \frac{\inp[\big]{\bar{\embed}_{\featset}}{\embed_{j}}}{\| \bar{\embed}_{\featset} \| \cdot \| \embed_{j} \|},
\end{equation}

where $\bar{\embed}_{\featset}$ is the mean of all vector embeddings of features in $\featset$ and $\embed_{f}$ is the vector embedding of the feature $f$. Here, the mean vector is supposed to represent a shared topic among words from $\featset$.


\subsection{Event detection}

We can now measure the similarity both in the time domain and in the semantic domain. These two measures are now combined into a single cost function $\cost$, which the algorithm will try to minimize:

\begin{equation} \label{eq:cost-function}
	\cost( \featset, f ) \coloneqq \frac{\trajdist( \featset, f )}{\exp(\semsim( \featset, f )) \cdot \sum_{g \in \featset \cup f}{\text{DPS}_{g}}}
\end{equation}

We exponentiate the cosine similarity so that the resulting value is always positive. Intuitively, a set $\featset \cup f$ which minimizes this function will have low inter-trajectory divergence, high semantic coherence and will be comprised of relevant features with high DPS.

To perform the event detection itself, we mostly adapt the \textit{Unsupervised greedy event detection} algorithm from \cite{event-detection}.

\begin{algorithm}[H]
\begin{algorithmic}[1]
\caption{Unsupervised greedy event detection}
\Input $\text{Feature set} ~ F = \text{HH or HL}$

\State $\text{Sort the features in ascending DPS order: } DPS_{f_{1}} \leq \dots \leq DPS_{f_{\left\vert F \right\vert}}$

\State $k = 0$

\ForEach{$f \in F$}
	\State $k = k + 1$	
	\State $e_{k} = \{ f \}$
	\State $cost_{e_{k}} = \frac{1}{DPS_{f}}$
	\State $F = F \setminus f$
	
	\While{$F \neq \emptyset$}
		\State $m = \argmin\limits_{m}{\cost( e_{k}, f_{m} )}$

		\If{$\cost( e_{k}, f_{m} ) < cost_{e_{k}}$}
			\State $cost_{e_{k}} = \cost( e_{k}, f_{m} )$
			\State $e_{k} = e_{k} \cup f_{m}$
			\State $F = F \setminus f_{m}$
		\Else
			\Break
		\EndIf
	\EndWhile
\EndFor

\Output $\text{Events} ~ \{ e_{1}, e_{2}, \dots, e_{k} \}$
\end{algorithmic}
\end{algorithm}

We do make a change and sort the features in \textit{ascending} rather than descending order. This ensures that words with lower DPS value will get selected first and that the function \ref{eq:cost-function} will not be minimized as quickly. As a result, the events will contain more representative features and will not be broken into several parts. This effectively relaxes the DPS part of the function \ref{eq:cost-function} while keeping emphasis on the trajectory similarity and document overlap.