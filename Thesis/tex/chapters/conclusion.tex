We examined how word embedding models, namely the Word2Vec model \citep{word2vec}, can be used to improve event detection quality. We tried to augment an existing method to use Word2Vec-based similarity function as a semantic measure to match keywords together into events. This did not bring significant improvement -- although the detected events were richer and less redundant, a notable amount of noise appeared. This made the events hard to assign to their real world counterparts, as most of the words did not contribute to any underlying topic.

Additionally, we tried a different approach by defining a distance function also using the Word2Vec model. We then applied a clustering algorithm to cluster the words using this distance function, and interpreted the clusters as individual events. Our evaluation suggests that this method was more successful than both the original method and its Word2Vec modification. The resulting events were composed mostly of representative words and reached lesser redundancy and noisiness.

The disadvantage of our methods is the necessity to train the Word2Vec model, which is time consuming. However, it can be trained once and than reused for additional detections, as long as the vocabulary remains similar.

We also examined how the Word2Vec model could be used to retrieve the documents concerning the detected events. We applied the Word Mover's Distance \citep{wmd} to documents within each event's bursty period as a measure of their similarity to that particular event's topic. We then selected a number of most relevant documents as the event's document representation. Although the documents were of high quality and described the event nicely, the process took an unbearable amount of time. In the original method, the retrieval process was more straightforward and much more efficient.

Finally, we applied multi-document summarization techniques to the documents to obtain a short annotation describing each event. This, along with intervals of the event's occurrence dates and document sets, are the outputs of our method presented to the user. They serve the purpose of giving a quick reference of the event's topic, based on which the user may decide to examine the event further using the retrieved documents.

In future work, it would be beneficial to use a more efficient way of computing the documents relevant to each event. Traditional information retrieval techniques, such as Latent Semantic Indexing \citep{lsi} could be used here, perhaps with some domain specific knowledge of the underlying events, such as their bursty periods.

Also, we would like to examine how an event could be represented directly as a set of documents, rather than words. Although there are attempts to do so \citep{document-bursty-representation}, they require to fine-tune a number of parameters, and the document representation is again constructed using word trajectories. The Doc2Vec model \citep{doc2vec}, a generalization of Word2Vec able to embed whole documents in a vector space, could be used to obtain the semantic representation.

Instead of computing a cutoff value to clean the word or event trajectory, as we did in \autoref{chap:event-detection}, further signal processing techniques could be applied on the trajectories to separate the dominant bursts from the underlying noise. The result would be a somewhat cleaner trajectory devoid of any milder bursts of no interest. This could lower the noisiness, since words would be matched together based on only the dominant activity, not any underlying influence, which still eludes the cutoff value method.