In this thesis, we addressed retrospective event detection from text streams. The task is to analyze a given document collection and uncover real events that happened over the stream period. These events are described by semantically related keywords whose occurrence in the stream has similar temporal characteristic. Once the events have been found, the relevant documents can be recovered by querying the documents with the event keywords. We focused on various ways to augment the event detection by using Word2Vec model \citep{word2vec} to measure the word semantic similarity as well as retrieve the documents.

First, we attempted to augment an existing method by \cite{event-detection} to use a Word2Vec-based similarity function to match semantically related words together. This led to an improvement over the original method, mainly in the average number of keywords per event. The modified method reached lower detection redundancy and the found events were less noisy. However, the method declined in precision and recall.

Then, we explored a different approach, where we interpreted the keyword-based event detection as a literal clustering task. We defined a custom distance function also utilizing the Word2Vec model as a semantic measure. We then applied a clustering algorithm equipped with this distance function to words previously selected as eventful. Our evaluation suggests that this method was more successful than both the original method and its Word2Vec modification. The cluster-based method reached even lower redundancy and noisiness than the previous methods while surpassing them in precision and recall.

The disadvantage of both our methods is the necessity to train the Word2Vec model, which is time consuming. However, the Word2Vec  model supports online learning; the training can be stopped and resumed as necessary. When we are about to detect events from a different document collection with a distinct vocabulary, we can simply embed the new words prior to starting the algorithm.

We also examined how the Word2Vec model could be used to retrieve documents concerning the detected events. We applied the Word Mover's Distance \citep{wmd} to documents within each event's bursty period as a measure of their relevance to that particular event's keyword set. We then selected the most relevant documents as the event's document representation. Although the documents were of high quality and represented the events well, the process took an unbearable amount of time. In the original method, the retrieval process was more straightforward and much more efficient.

Finally, we applied multi-document summarization techniques to the documents to obtain a short annotation describing each event. These summaries, along with the event's occurrence dates and document sets, are the outputs of our method presented to the user. The summaries serve the purpose of giving a quick reference of the event's topic, based on which the user may decide to examine the event further and go through the retrieved documents.

In future work, it would be beneficial to use a more efficient way of computing the documents relevant to each event. Traditional information retrieval techniques, such as Latent Semantic Indexing \citep{lsi} could be used here, perhaps with some domain specific knowledge of the underlying events, such as their bursty periods.

Also, we would like to examine how an event could be represented directly as a set of documents, rather than keywords. Although there are attempts to do so \citep{document-bursty-representation}, they require to fine-tune even more parameters than our method, and the document representation is again constructed using word trajectories. The Doc2Vec model \citep{doc2vec}, a generalization of Word2Vec able to embed whole documents in a vector space, could be used to obtain the semantic representation.

Instead of computing a cutoff value to clean a word or an event trajectory, as we did in \autoref{subsec:noise-filtering} and \autoref{subsec:trajectory-filtering}, further signal processing techniques could be applied on the trajectories to separate the dominant bursts from the underlying noise. The result would be a somewhat cleaner trajectory devoid of any milder bursts of no interest. This could lower the noisiness, since words would be matched together based on only the dominant activity, not any underlying influence, which still eludes the cutoff value method.